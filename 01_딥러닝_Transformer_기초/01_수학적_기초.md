# Chapter 1.1: 수학적 기초

## 개요

Transformer와 딥러닝을 이해하기 위한 핵심 수학적 개념을 다룬다.

---

## 1. 선형대수 (Linear Algebra)

### 1.1 행렬 연산

#### 행렬 곱셈 (Matrix Multiplication)

Transformer의 핵심 연산은 행렬 곱셈이다. 두 행렬 A(m×n)와 B(n×p)의 곱:

```
C = AB, where C[i,j] = Σ(k=1 to n) A[i,k] × B[k,j]
```

**Attention 연산에서의 활용:**
```python
# Q, K, V 행렬과 Attention Score 계산
# Q: (batch, seq_len, d_k)
# K: (batch, seq_len, d_k)
# V: (batch, seq_len, d_v)

attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)
attention_output = torch.matmul(attention_weights, V)     # (batch, seq_len, d_v)
```

#### 전치 행렬 (Transpose)

행렬의 행과 열을 교환하는 연산:

```
A^T[i,j] = A[j,i]
```

Attention에서 K 행렬을 전치하여 Q와 내적 계산:
```python
K_transposed = K.transpose(-2, -1)
scores = Q @ K_transposed
```

### 1.2 벡터 연산

#### 내적 (Dot Product)

두 벡터 간의 유사도 측정에 사용:

```
a · b = Σ(i=1 to n) a[i] × b[i] = ||a|| × ||b|| × cos(θ)
```

**의미:**
- 두 벡터가 같은 방향: 큰 양수 값
- 직교 (orthogonal): 0
- 반대 방향: 큰 음수 값

#### 코사인 유사도 (Cosine Similarity)

임베딩 벡터 간 의미적 유사도 측정:

```
cos(θ) = (a · b) / (||a|| × ||b||)
```

**범위:** -1 (반대) ~ 1 (동일)

```python
def cosine_similarity(a, b):
    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))
```

### 1.3 고유값과 고유벡터 (Eigenvalue & Eigenvector)

행렬 A에 대해:
```
Av = λv
```
- **v**: 고유벡터 (eigenvector)
- **λ**: 고유값 (eigenvalue)

**Attention 이해에 유용한 이유:**
- Self-attention은 입력의 선형 변환
- 고유벡터는 변환에서 방향이 보존되는 벡터
- 고유값은 해당 방향의 스케일링 정도

---

## 2. 미적분 (Calculus)

### 2.1 Chain Rule (연쇄 법칙)

**Backpropagation의 핵심 원리**

합성 함수의 미분:
```
d/dx[f(g(x))] = f'(g(x)) × g'(x)
```

**다변수 확장:**
```
∂L/∂w = (∂L/∂y) × (∂y/∂w)
```

**신경망에서의 적용:**

```python
# Forward pass
z = W @ x + b      # 선형 변환
a = relu(z)        # 활성화 함수
L = loss(a, y)     # 손실 함수

# Backward pass (Chain Rule 적용)
dL_da = d_loss(a, y)           # ∂L/∂a
da_dz = d_relu(z)               # ∂a/∂z
dz_dW = x                       # ∂z/∂W

dL_dW = dL_da @ da_dz @ dz_dW  # 연쇄 법칙
```

### 2.2 Gradient (기울기)

다변수 함수의 편미분 벡터:

```
∇f(x) = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]
```

**특성:**
- 함수가 가장 빠르게 증가하는 방향
- Gradient Descent는 -∇f 방향으로 이동

### 2.3 Jacobian 행렬

벡터 함수의 편미분 행렬:

```
J = [∂f₁/∂x₁  ∂f₁/∂x₂  ...  ∂f₁/∂xₙ]
    [∂f₂/∂x₁  ∂f₂/∂x₂  ...  ∂f₂/∂xₙ]
    [   ⋮        ⋮      ⋱      ⋮   ]
    [∂fₘ/∂x₁  ∂fₘ/∂x₂  ...  ∂fₘ/∂xₙ]
```

**신경망에서 활용:**
- 층 간 gradient 전파
- Batch normalization의 gradient 계산

### 2.4 Softmax의 미분

Softmax 함수:
```
softmax(xᵢ) = exp(xᵢ) / Σⱼ exp(xⱼ)
```

**Jacobian:**
```
∂softmax(xᵢ)/∂xⱼ = softmax(xᵢ)(δᵢⱼ - softmax(xⱼ))
```

여기서 δᵢⱼ는 Kronecker delta (i=j이면 1, 아니면 0)

```python
def softmax_jacobian(s):
    """
    s: softmax output (n,)
    returns: Jacobian matrix (n, n)
    """
    n = len(s)
    jacobian = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            if i == j:
                jacobian[i, j] = s[i] * (1 - s[i])
            else:
                jacobian[i, j] = -s[i] * s[j]
    return jacobian
```

---

## 3. 확률과 통계 (Probability & Statistics)

### 3.1 확률 분포

#### Gaussian (Normal) Distribution

```
p(x) = (1 / √(2πσ²)) × exp(-(x-μ)² / 2σ²)
```

**신경망에서 활용:**
- 가중치 초기화
- Dropout
- Variational Autoencoder

#### Categorical Distribution

이산 확률 분포:
```
p(x = k) = πₖ, where Σₖ πₖ = 1
```

**언어 모델에서:**
- 다음 토큰 예측 확률
- Softmax 출력이 categorical distribution 형성

### 3.2 Cross-Entropy Loss

두 확률 분포 간의 차이 측정:

```
H(p, q) = -Σᵢ p(i) × log(q(i))
```

**분류 문제에서:**
- p: 실제 레이블 (one-hot encoding)
- q: 모델의 예측 확률

```python
def cross_entropy_loss(y_true, y_pred):
    """
    y_true: one-hot encoded labels (batch, num_classes)
    y_pred: predicted probabilities (batch, num_classes)
    """
    epsilon = 1e-10  # numerical stability
    return -torch.sum(y_true * torch.log(y_pred + epsilon), dim=-1).mean()
```

**Language Model에서:**
```python
# Next token prediction
# logits: (batch, seq_len, vocab_size)
# labels: (batch, seq_len)
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    labels.view(-1)
)
```

### 3.3 KL Divergence

두 확률 분포 간의 정보 이론적 거리:

```
D_KL(P || Q) = Σᵢ P(i) × log(P(i) / Q(i))
```

**특성:**
- 항상 ≥ 0
- P = Q일 때만 0
- 비대칭: D_KL(P||Q) ≠ D_KL(Q||P)

**활용:**
- Knowledge Distillation
- VAE의 regularization term
- Policy Gradient (강화학습)

```python
def kl_divergence(p, q):
    """
    p, q: probability distributions
    """
    return torch.sum(p * torch.log(p / (q + 1e-10)), dim=-1)
```

---

## 4. 실습: NumPy로 Neural Network 구현

### 4.1 Forward Pass

```python
import numpy as np

class SimpleNN:
    def __init__(self, input_size, hidden_size, output_size):
        # Xavier initialization
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
        self.b2 = np.zeros((1, output_size))

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    def forward(self, x):
        # Layer 1
        self.z1 = x @ self.W1 + self.b1
        self.a1 = self.relu(self.z1)

        # Layer 2
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = self.softmax(self.z2)

        return self.a2
```

### 4.2 Backward Pass (Chain Rule 적용)

```python
def backward(self, x, y_true, learning_rate=0.01):
    batch_size = x.shape[0]

    # Output layer gradient
    # ∂L/∂z2 = a2 - y_true (for cross-entropy with softmax)
    dz2 = self.a2 - y_true

    # ∂L/∂W2 = a1^T @ dz2
    dW2 = (self.a1.T @ dz2) / batch_size
    db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size

    # Hidden layer gradient (Chain Rule)
    # ∂L/∂a1 = dz2 @ W2^T
    da1 = dz2 @ self.W2.T

    # ∂L/∂z1 = da1 * relu'(z1)
    dz1 = da1 * self.relu_derivative(self.z1)

    # ∂L/∂W1 = x^T @ dz1
    dW1 = (x.T @ dz1) / batch_size
    db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size

    # Update weights
    self.W2 -= learning_rate * dW2
    self.b2 -= learning_rate * db2
    self.W1 -= learning_rate * dW1
    self.b1 -= learning_rate * db1
```

### 4.3 전체 학습 루프

```python
def train(self, X, Y, epochs=1000, learning_rate=0.01):
    losses = []

    for epoch in range(epochs):
        # Forward
        predictions = self.forward(X)

        # Loss (Cross-Entropy)
        loss = -np.mean(np.sum(Y * np.log(predictions + 1e-10), axis=1))
        losses.append(loss)

        # Backward
        self.backward(X, Y, learning_rate)

        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    return losses

# 사용 예시
nn = SimpleNN(input_size=784, hidden_size=128, output_size=10)
losses = nn.train(X_train, Y_train, epochs=1000, learning_rate=0.1)
```

---

## 핵심 참고 자료

### 교재
- **"Deep Learning"** by Goodfellow, Bengio, Courville
  - Chapter 2: Linear Algebra
  - Chapter 4: Numerical Computation
  - Chapter 6.5: Back-Propagation

### 강의
- **Stanford CS231n**: Backpropagation and Neural Networks
  - https://cs231n.github.io/optimization-2/

- **3Blue1Brown**: Essence of Linear Algebra (YouTube Series)
  - 선형대수 시각화 강의

### 논문
- **Gradient Checkpointing**: Chen et al. (2016). "Training Deep Nets with Sublinear Memory Cost"
  - arXiv:1604.06174
  - 메모리 효율적인 backpropagation 기법

---

## 핵심 요약

| 개념 | 역할 | Transformer에서 활용 |
|------|------|---------------------|
| 행렬 곱셈 | 선형 변환 | Q, K, V 계산, Attention 연산 |
| 내적 | 유사도 측정 | Attention Score 계산 |
| Chain Rule | Gradient 전파 | Backpropagation |
| Softmax | 확률 분포 변환 | Attention Weights, 출력 확률 |
| Cross-Entropy | 손실 함수 | 분류/언어 모델 학습 |

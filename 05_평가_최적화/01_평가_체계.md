# Chapter 5.1: 평가 체계 구축

## 개요

VLM 및 OCR 모델의 성능을 정량적으로 평가하는 메트릭과 방법론을 다룬다.

---

## 1. OCR 메트릭

### 1.1 Character Error Rate (CER)

문자 단위 오류율:

```
CER = (S + D + I) / N

S: Substitution (대체)
D: Deletion (삭제)
I: Insertion (삽입)
N: Ground truth 총 문자 수
```

```python
def calculate_cer(prediction: str, ground_truth: str) -> float:
    """
    Character Error Rate 계산 (Levenshtein Distance 기반)
    """
    import numpy as np

    pred = list(prediction)
    gt = list(ground_truth)

    m, n = len(pred), len(gt)

    # DP 테이블
    dp = np.zeros((m + 1, n + 1))

    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if pred[i-1] == gt[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(
                    dp[i-1][j],      # Deletion
                    dp[i][j-1],      # Insertion
                    dp[i-1][j-1]     # Substitution
                )

    edit_distance = dp[m][n]
    cer = edit_distance / len(gt) if len(gt) > 0 else 0.0

    return cer


# 사용 예시
pred = "환자명: 홍길덩"
gt = "환자명: 홍길동"
print(f"CER: {calculate_cer(pred, gt):.2%}")  # CER: 10.00%
```

### 1.2 Word Error Rate (WER)

단어 단위 오류율:

```python
def calculate_wer(prediction: str, ground_truth: str) -> float:
    """
    Word Error Rate 계산
    """
    pred_words = prediction.split()
    gt_words = ground_truth.split()

    # CER과 동일한 알고리즘, 단위만 단어
    import numpy as np

    m, n = len(pred_words), len(gt_words)
    dp = np.zeros((m + 1, n + 1))

    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if pred_words[i-1] == gt_words[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])

    wer = dp[m][n] / len(gt_words) if len(gt_words) > 0 else 0.0
    return wer
```

### 1.3 CER vs WER 비교

| 메트릭 | 민감도 | 적합한 상황 |
|--------|--------|------------|
| CER | 문자 단위 | 타이핑, 코드, 숫자 |
| WER | 단어 단위 | 자연어, 검색 |

---

## 2. 구조화 출력 평가

### 2.1 JSON Schema 일치율

```python
from jsonschema import validate, ValidationError
import json

def evaluate_json_output(prediction: str, schema: dict) -> dict:
    """
    JSON 출력의 스키마 일치 여부 평가
    """
    result = {
        "valid_json": False,
        "schema_valid": False,
        "parsed": None,
        "errors": []
    }

    # JSON 파싱
    try:
        parsed = json.loads(prediction)
        result["valid_json"] = True
        result["parsed"] = parsed
    except json.JSONDecodeError as e:
        result["errors"].append(f"JSON parse error: {e}")
        return result

    # 스키마 검증
    try:
        validate(instance=parsed, schema=schema)
        result["schema_valid"] = True
    except ValidationError as e:
        result["errors"].append(f"Schema validation error: {e.message}")

    return result


# 사용 예시
schema = {
    "type": "object",
    "required": ["patient_name", "diagnosis", "date"],
    "properties": {
        "patient_name": {"type": "string"},
        "diagnosis": {"type": "string"},
        "date": {"type": "string", "pattern": r"^\d{4}-\d{2}-\d{2}$"}
    }
}

prediction = '{"patient_name": "홍길동", "diagnosis": "감기", "date": "2024-01-20"}'
result = evaluate_json_output(prediction, schema)
```

### 2.2 Key-Value 정확도

```python
def evaluate_key_value_accuracy(
    prediction: dict,
    ground_truth: dict,
    exact_match: bool = True
) -> dict:
    """
    Key-Value 추출 정확도 평가
    """
    results = {
        "total_keys": len(ground_truth),
        "correct": 0,
        "missing": [],
        "wrong": [],
        "extra": []
    }

    for key, gt_value in ground_truth.items():
        if key not in prediction:
            results["missing"].append(key)
        else:
            pred_value = prediction[key]

            if exact_match:
                is_correct = str(pred_value).strip() == str(gt_value).strip()
            else:
                # Fuzzy matching
                is_correct = fuzzy_match(str(pred_value), str(gt_value), threshold=0.9)

            if is_correct:
                results["correct"] += 1
            else:
                results["wrong"].append({
                    "key": key,
                    "predicted": pred_value,
                    "expected": gt_value
                })

    # 추가 키 확인
    for key in prediction:
        if key not in ground_truth:
            results["extra"].append(key)

    results["accuracy"] = results["correct"] / results["total_keys"] if results["total_keys"] > 0 else 0
    results["recall"] = results["correct"] / results["total_keys"] if results["total_keys"] > 0 else 0
    results["precision"] = results["correct"] / len(prediction) if len(prediction) > 0 else 0

    return results


def fuzzy_match(pred: str, gt: str, threshold: float = 0.9) -> bool:
    """퍼지 매칭"""
    from rapidfuzz import fuzz
    return fuzz.ratio(pred, gt) / 100 >= threshold
```

### 2.3 테이블 셀 정확도

```python
def evaluate_table_extraction(
    pred_table: list,
    gt_table: list
) -> dict:
    """
    테이블 추출 정확도 평가

    pred_table, gt_table: 2D list (rows × cols)
    """
    if not gt_table:
        return {"error": "Empty ground truth table"}

    gt_rows = len(gt_table)
    gt_cols = len(gt_table[0]) if gt_table else 0
    pred_rows = len(pred_table) if pred_table else 0
    pred_cols = len(pred_table[0]) if pred_table and pred_table[0] else 0

    # 구조 일치
    structure_match = (gt_rows == pred_rows) and (gt_cols == pred_cols)

    # 셀 단위 정확도
    correct_cells = 0
    total_cells = gt_rows * gt_cols

    for i in range(min(gt_rows, pred_rows)):
        for j in range(min(gt_cols, pred_cols)):
            gt_cell = str(gt_table[i][j]).strip()
            pred_cell = str(pred_table[i][j]).strip() if i < pred_rows and j < pred_cols else ""

            if gt_cell == pred_cell:
                correct_cells += 1

    return {
        "structure_match": structure_match,
        "cell_accuracy": correct_cells / total_cells if total_cells > 0 else 0,
        "correct_cells": correct_cells,
        "total_cells": total_cells,
        "gt_shape": (gt_rows, gt_cols),
        "pred_shape": (pred_rows, pred_cols)
    }
```

---

## 3. 의료 문서 특화 평가

### 3.1 필수 필드 추출률

```python
MEDICAL_REQUIRED_FIELDS = {
    "진단서": ["환자명", "생년월일", "진단명", "진단일", "의사명"],
    "처방전": ["환자명", "약품명", "용량", "투약일수"],
    "영수증": ["환자명", "총액", "진료일"]
}

def evaluate_medical_extraction(
    prediction: dict,
    ground_truth: dict,
    document_type: str
) -> dict:
    """
    의료 문서 필수 필드 추출 평가
    """
    required_fields = MEDICAL_REQUIRED_FIELDS.get(document_type, [])

    results = {
        "document_type": document_type,
        "required_fields": required_fields,
        "extracted": {},
        "missing": [],
        "accuracy": {}
    }

    for field in required_fields:
        gt_value = ground_truth.get(field)
        pred_value = prediction.get(field)

        if pred_value is None:
            results["missing"].append(field)
        else:
            results["extracted"][field] = pred_value

            # 정확도 검사
            if gt_value is not None:
                is_correct = str(pred_value).strip() == str(gt_value).strip()
                results["accuracy"][field] = is_correct

    # 전체 통계
    results["extraction_rate"] = len(results["extracted"]) / len(required_fields)
    results["accuracy_rate"] = sum(results["accuracy"].values()) / len(required_fields) if required_fields else 0

    return results
```

### 3.2 숫자 정확도 (금액, 용량)

```python
import re

def evaluate_numeric_accuracy(pred_value: str, gt_value: str, tolerance: float = 0.01) -> dict:
    """
    숫자 값 정확도 평가 (금액, 용량 등)
    """
    # 숫자 추출
    def extract_number(text: str) -> float:
        # 콤마, 원, mg 등 제거
        cleaned = re.sub(r'[^\d.]', '', str(text).replace(',', ''))
        try:
            return float(cleaned)
        except:
            return None

    pred_num = extract_number(pred_value)
    gt_num = extract_number(gt_value)

    if pred_num is None or gt_num is None:
        return {
            "match": False,
            "error": "Could not parse number",
            "pred": pred_value,
            "gt": gt_value
        }

    # 허용 오차 내 일치
    if gt_num == 0:
        match = pred_num == 0
    else:
        relative_error = abs(pred_num - gt_num) / abs(gt_num)
        match = relative_error <= tolerance

    return {
        "match": match,
        "pred_value": pred_num,
        "gt_value": gt_num,
        "relative_error": abs(pred_num - gt_num) / abs(gt_num) if gt_num != 0 else 0
    }
```

---

## 4. 평가 파이프라인

### 4.1 통합 평가 클래스

```python
from dataclasses import dataclass, field
from typing import List, Dict, Optional
import json

@dataclass
class EvaluationResult:
    """평가 결과 데이터 클래스"""
    sample_id: str
    cer: float
    wer: float
    json_valid: bool
    field_accuracy: Dict[str, bool]
    numeric_accuracy: Dict[str, bool]
    overall_score: float = 0.0

class VLMEvaluator:
    """VLM 종합 평가기"""

    def __init__(self, config: dict):
        self.config = config
        self.results: List[EvaluationResult] = []

    def evaluate_sample(
        self,
        sample_id: str,
        prediction: str,
        ground_truth: dict,
        document_type: str
    ) -> EvaluationResult:
        """단일 샘플 평가"""

        # 1. OCR 메트릭
        gt_text = ground_truth.get("full_text", "")
        cer = calculate_cer(prediction, gt_text) if gt_text else 0.0
        wer = calculate_wer(prediction, gt_text) if gt_text else 0.0

        # 2. JSON 파싱
        json_result = evaluate_json_output(prediction, self.config.get("schema", {}))

        # 3. 필드 정확도
        field_accuracy = {}
        numeric_accuracy = {}

        if json_result["valid_json"] and json_result["parsed"]:
            parsed = json_result["parsed"]
            gt_fields = ground_truth.get("fields", {})

            for field, gt_value in gt_fields.items():
                pred_value = parsed.get(field)

                if pred_value is not None:
                    # 숫자 필드
                    if field in self.config.get("numeric_fields", []):
                        num_result = evaluate_numeric_accuracy(str(pred_value), str(gt_value))
                        numeric_accuracy[field] = num_result["match"]
                        field_accuracy[field] = num_result["match"]
                    else:
                        field_accuracy[field] = str(pred_value).strip() == str(gt_value).strip()
                else:
                    field_accuracy[field] = False

        # 4. 종합 점수
        overall_score = self._calculate_overall_score(
            cer, wer, json_result["valid_json"], field_accuracy
        )

        result = EvaluationResult(
            sample_id=sample_id,
            cer=cer,
            wer=wer,
            json_valid=json_result["valid_json"],
            field_accuracy=field_accuracy,
            numeric_accuracy=numeric_accuracy,
            overall_score=overall_score
        )

        self.results.append(result)
        return result

    def _calculate_overall_score(
        self,
        cer: float,
        wer: float,
        json_valid: bool,
        field_accuracy: dict
    ) -> float:
        """종합 점수 계산"""
        weights = self.config.get("weights", {
            "cer": 0.2,
            "wer": 0.1,
            "json": 0.2,
            "fields": 0.5
        })

        score = 0.0
        score += (1 - cer) * weights["cer"]
        score += (1 - wer) * weights["wer"]
        score += float(json_valid) * weights["json"]

        if field_accuracy:
            field_score = sum(field_accuracy.values()) / len(field_accuracy)
            score += field_score * weights["fields"]

        return score

    def generate_report(self) -> dict:
        """평가 리포트 생성"""
        if not self.results:
            return {"error": "No results"}

        return {
            "total_samples": len(self.results),
            "average_cer": sum(r.cer for r in self.results) / len(self.results),
            "average_wer": sum(r.wer for r in self.results) / len(self.results),
            "json_valid_rate": sum(r.json_valid for r in self.results) / len(self.results),
            "average_overall_score": sum(r.overall_score for r in self.results) / len(self.results),
            "field_accuracy_by_field": self._aggregate_field_accuracy()
        }

    def _aggregate_field_accuracy(self) -> dict:
        """필드별 정확도 집계"""
        field_stats = {}

        for result in self.results:
            for field, correct in result.field_accuracy.items():
                if field not in field_stats:
                    field_stats[field] = {"correct": 0, "total": 0}
                field_stats[field]["total"] += 1
                field_stats[field]["correct"] += int(correct)

        return {
            field: stats["correct"] / stats["total"]
            for field, stats in field_stats.items()
        }
```

---

## 5. Human Evaluation

### 5.1 평가 프로토콜

```python
@dataclass
class HumanEvalCriteria:
    """Human Evaluation 기준"""
    completeness: int  # 1-5: 필수 정보 포함 여부
    accuracy: int      # 1-5: 정보 정확성
    formatting: int    # 1-5: 출력 형식 적절성
    readability: int   # 1-5: 가독성
    critical_errors: List[str]  # 치명적 오류 목록

class HumanEvaluator:
    """Human Evaluation 수집"""

    def __init__(self, criteria_weights: dict = None):
        self.criteria_weights = criteria_weights or {
            "completeness": 0.3,
            "accuracy": 0.4,
            "formatting": 0.15,
            "readability": 0.15
        }
        self.evaluations = []

    def collect_evaluation(
        self,
        sample_id: str,
        prediction: str,
        criteria: HumanEvalCriteria
    ) -> float:
        """평가 수집 및 점수 계산"""

        # 가중 평균
        score = (
            criteria.completeness * self.criteria_weights["completeness"] +
            criteria.accuracy * self.criteria_weights["accuracy"] +
            criteria.formatting * self.criteria_weights["formatting"] +
            criteria.readability * self.criteria_weights["readability"]
        ) / 5.0  # 1-5 스케일을 0-1로 변환

        # 치명적 오류가 있으면 감점
        if criteria.critical_errors:
            score *= 0.5

        self.evaluations.append({
            "sample_id": sample_id,
            "criteria": criteria,
            "score": score
        })

        return score
```

---

## 핵심 참고 자료

### 메트릭
- **CER/WER**: Levenshtein Distance 기반
- **BLEU**: https://aclanthology.org/P02-1040/
- **ROUGE**: https://aclanthology.org/W04-1013/

### 벤치마크
- **DocVQA**: https://www.docvqa.org/
- **ChartQA**: https://github.com/vis-nlp/ChartQA
- **TextVQA**: https://textvqa.org/

---

## 핵심 요약

| 메트릭 | 측정 대상 | 범위 | 목표 |
|--------|----------|------|------|
| CER | 문자 오류 | 0-∞ | < 5% |
| WER | 단어 오류 | 0-∞ | < 10% |
| Field Accuracy | 필드 추출 | 0-100% | > 95% |
| JSON Valid | 형식 준수 | 0/1 | 100% |

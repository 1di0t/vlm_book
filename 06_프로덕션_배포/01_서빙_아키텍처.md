# Chapter 6.1: 서빙 아키텍처

## 개요

VLM의 프로덕션 배포를 위한 서빙 아키텍처, vLLM/SGLang 활용, 모니터링 전략을 다룬다.

---

## 1. 추론 엔진 비교

### 1.1 주요 추론 엔진

| 엔진 | 핵심 기술 | 장점 | 단점 |
|------|----------|------|------|
| **vLLM** | PagedAttention | 높은 처리량, 쉬운 사용 | VLM 지원 제한적 |
| **SGLang** | RadixAttention | KV Cache 재사용 | 학습 곡선 |
| **TensorRT-LLM** | NVIDIA 최적화 | 최고 성능 | NVIDIA 종속 |
| **HuggingFace TGI** | 범용성 | 사용 편의 | 성능 한계 |

---

## 2. vLLM 활용

> **논문**: Kwon et al. (2023). "Efficient Memory Management for Large Language Model Serving with PagedAttention"
> - arXiv: https://arxiv.org/abs/2309.06180
> - SOSP 2023

### 2.1 PagedAttention 원리

```
기존: 연속 메모리 할당 → 60-80% 메모리 낭비 (fragmentation)
vLLM: 페이지 단위 동적 할당 → < 4% 메모리 낭비
```

### 2.2 vLLM 서버 설정

```python
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# 동기 방식
def setup_vllm_sync(model_path: str):
    """vLLM 동기 엔진 설정"""
    llm = LLM(
        model=model_path,
        tensor_parallel_size=1,           # GPU 수
        gpu_memory_utilization=0.9,       # GPU 메모리 사용률
        max_model_len=4096,               # 최대 시퀀스 길이
        trust_remote_code=True,
        dtype="bfloat16"
    )
    return llm


# 비동기 방식 (프로덕션 권장)
async def setup_vllm_async(model_path: str):
    """vLLM 비동기 엔진 설정"""
    engine_args = AsyncEngineArgs(
        model=model_path,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        max_model_len=4096,
        enable_prefix_caching=True,       # Prefix caching 활성화
        max_num_batched_tokens=32768,
        max_num_seqs=256
    )
    engine = AsyncLLMEngine.from_engine_args(engine_args)
    return engine


# 추론 실행
def generate_with_vllm(llm, prompts: list, max_tokens: int = 1024):
    """vLLM 추론"""
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=max_tokens
    )

    outputs = llm.generate(prompts, sampling_params)

    results = []
    for output in outputs:
        generated_text = output.outputs[0].text
        results.append(generated_text)

    return results
```

### 2.3 vLLM + FastAPI 서버

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import asyncio
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams

app = FastAPI(title="VLM Serving API")

# Global engine
engine: Optional[AsyncLLMEngine] = None

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9

class GenerateResponse(BaseModel):
    generated_text: str
    num_tokens: int
    finish_reason: str

@app.on_event("startup")
async def startup():
    global engine
    engine = await setup_vllm_async("Qwen/Qwen2-VL-7B-Instruct")

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    if engine is None:
        raise HTTPException(status_code=503, detail="Engine not initialized")

    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=request.top_p,
        max_tokens=request.max_tokens
    )

    # 요청 ID 생성
    request_id = str(uuid.uuid4())

    # 비동기 생성
    results_generator = engine.generate(
        request.prompt,
        sampling_params,
        request_id
    )

    final_output = None
    async for request_output in results_generator:
        final_output = request_output

    if final_output is None:
        raise HTTPException(status_code=500, detail="Generation failed")

    output = final_output.outputs[0]

    return GenerateResponse(
        generated_text=output.text,
        num_tokens=len(output.token_ids),
        finish_reason=output.finish_reason
    )

@app.get("/health")
async def health_check():
    return {"status": "healthy", "engine_loaded": engine is not None}

# 실행: uvicorn main:app --host 0.0.0.0 --port 8000
```

---

## 3. SGLang 활용

> **논문**: Zheng et al. (2024). "SGLang: Efficient Execution of Structured Language Model Programs"
> - arXiv: https://arxiv.org/abs/2312.07104
> - NeurIPS 2024

### 3.1 RadixAttention

```
기존: 요청마다 KV Cache 폐기
SGLang: LRU 캐시로 KV Cache 재사용 (Radix Tree 구조)
```

**효과:**
- 반복 프롬프트에서 높은 캐시 히트율
- Prefill 시간 대폭 감소

### 3.2 SGLang 서버 설정

```bash
# 설치
pip install sglang[all]

# 서버 실행
python -m sglang.launch_server \
    --model-path Qwen/Qwen2-VL-7B-Instruct \
    --port 30000 \
    --tp 1 \
    --mem-fraction-static 0.8
```

### 3.3 SGLang 클라이언트

```python
import sglang as sgl

# Runtime 설정
sgl.set_default_backend(sgl.RuntimeEndpoint("http://localhost:30000"))

@sgl.function
def document_extraction(s, image_path: str, prompt: str):
    """SGLang 함수로 문서 추출 정의"""
    s += sgl.user(sgl.image(image_path) + prompt)
    s += sgl.assistant(sgl.gen("response", max_tokens=1024))

# 실행
result = document_extraction.run(
    image_path="document.png",
    prompt="이 문서의 내용을 JSON으로 추출해주세요."
)

print(result["response"])
```

---

## 4. Docker 배포

### 4.1 Dockerfile

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

# 기본 패키지
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# Python 패키지
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# vLLM 설치
RUN pip3 install vllm

# 애플리케이션 복사
WORKDIR /app
COPY . .

# 모델 다운로드 (선택적)
# RUN python3 download_model.py

# 포트 노출
EXPOSE 8000

# 실행
CMD ["python3", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 4.2 Docker Compose

```yaml
# docker-compose.yaml
version: '3.8'

services:
  vlm-server:
    build: .
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

---

## 5. 모니터링

### 5.1 Prometheus 메트릭

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# 메트릭 정의
REQUEST_COUNT = Counter(
    'vlm_requests_total',
    'Total number of requests',
    ['method', 'status']
)

REQUEST_LATENCY = Histogram(
    'vlm_request_latency_seconds',
    'Request latency in seconds',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

TOKENS_GENERATED = Counter(
    'vlm_tokens_generated_total',
    'Total number of tokens generated'
)

GPU_MEMORY_USAGE = Gauge(
    'vlm_gpu_memory_usage_bytes',
    'GPU memory usage in bytes',
    ['device']
)

ACTIVE_REQUESTS = Gauge(
    'vlm_active_requests',
    'Number of active requests'
)

class MetricsMiddleware:
    """FastAPI 메트릭 미들웨어"""

    async def __call__(self, request, call_next):
        start_time = time.time()
        ACTIVE_REQUESTS.inc()

        try:
            response = await call_next(request)
            REQUEST_COUNT.labels(method=request.method, status=response.status_code).inc()
            return response
        except Exception as e:
            REQUEST_COUNT.labels(method=request.method, status=500).inc()
            raise
        finally:
            ACTIVE_REQUESTS.dec()
            REQUEST_LATENCY.observe(time.time() - start_time)


def update_gpu_metrics():
    """GPU 메트릭 업데이트"""
    import torch
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            memory_used = torch.cuda.memory_allocated(i)
            GPU_MEMORY_USAGE.labels(device=f"cuda:{i}").set(memory_used)
```

### 5.2 로깅

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """구조화된 로깅"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)

    def log_request(self, request_id: str, prompt_length: int, **kwargs):
        """요청 로깅"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event": "request",
            "request_id": request_id,
            "prompt_length": prompt_length,
            **kwargs
        }
        self.logger.info(json.dumps(log_entry))

    def log_response(self, request_id: str, latency: float, tokens: int, **kwargs):
        """응답 로깅"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event": "response",
            "request_id": request_id,
            "latency_ms": latency * 1000,
            "tokens_generated": tokens,
            **kwargs
        }
        self.logger.info(json.dumps(log_entry))

    def log_error(self, request_id: str, error: str, **kwargs):
        """에러 로깅"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event": "error",
            "request_id": request_id,
            "error": error,
            **kwargs
        }
        self.logger.error(json.dumps(log_entry))
```

---

## 6. 성능 최적화

### 6.1 배치 처리

```python
class BatchProcessor:
    """동적 배치 처리"""

    def __init__(self, engine, max_batch_size: int = 32, max_wait_time: float = 0.1):
        self.engine = engine
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        self.lock = asyncio.Lock()

    async def add_request(self, prompt: str, sampling_params) -> str:
        """요청 추가"""
        future = asyncio.Future()

        async with self.lock:
            self.pending_requests.append({
                "prompt": prompt,
                "params": sampling_params,
                "future": future
            })

            if len(self.pending_requests) >= self.max_batch_size:
                await self._process_batch()

        return await future

    async def _process_batch(self):
        """배치 처리"""
        if not self.pending_requests:
            return

        batch = self.pending_requests[:self.max_batch_size]
        self.pending_requests = self.pending_requests[self.max_batch_size:]

        prompts = [r["prompt"] for r in batch]
        params = batch[0]["params"]  # 동일한 params 가정

        outputs = await self.engine.generate(prompts, params)

        for i, output in enumerate(outputs):
            batch[i]["future"].set_result(output.outputs[0].text)
```

### 6.2 성능 목표

| 메트릭 | 목표 | 측정 방법 |
|--------|------|----------|
| Latency (p50) | < 1초 | 단일 문서 |
| Latency (p99) | < 5초 | 복잡한 문서 |
| Throughput | > 100 req/min | 동시 요청 |
| GPU Utilization | > 80% | nvidia-smi |

---

## 핵심 참고 자료

### 논문
- **vLLM** (Kwon et al., 2023)
  - https://arxiv.org/abs/2309.06180
  - GitHub: https://github.com/vllm-project/vllm

- **SGLang** (Zheng et al., 2024)
  - https://arxiv.org/abs/2312.07104
  - GitHub: https://github.com/sgl-project/sglang

### 도구
- **vLLM Documentation**: https://docs.vllm.ai/
- **TensorRT-LLM**: https://github.com/NVIDIA/TensorRT-LLM
- **Prometheus**: https://prometheus.io/
- **Grafana**: https://grafana.com/

---

## 핵심 요약

| 단계 | 도구 | 핵심 설정 |
|------|------|----------|
| 추론 엔진 | vLLM / SGLang | PagedAttention, RadixAttention |
| API 서버 | FastAPI | 비동기, 배치 처리 |
| 컨테이너 | Docker | GPU 지원, 헬스체크 |
| 모니터링 | Prometheus + Grafana | 지연시간, 처리량, GPU |

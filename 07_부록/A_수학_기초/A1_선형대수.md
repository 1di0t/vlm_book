# A1. ì„ í˜•ëŒ€ìˆ˜ (Linear Algebra)

> Transformerì™€ Neural Network ì—°ì‚°ì˜ ìˆ˜í•™ì  ê¸°ë°˜

---

## 1. ë²¡í„°ì™€ í–‰ë ¬

### 1.1 ê¸°ë³¸ ì •ì˜

**ìŠ¤ì¹¼ë¼ (Scalar)**: ë‹¨ì¼ ìˆ«ì
```
x = 3.14
```

**ë²¡í„° (Vector)**: ìˆ«ìì˜ ìˆœì„œ ìˆëŠ” ë°°ì—´
```
v = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€ âˆˆ â„â¿

ì˜ˆ: ë‹¨ì–´ ì„ë² ë”© [0.2, -0.5, 0.8, ...]
```

**í–‰ë ¬ (Matrix)**: 2ì°¨ì› ìˆ«ì ë°°ì—´
```
A = [aâ‚â‚  aâ‚â‚‚  ...  aâ‚â‚™]
    [aâ‚‚â‚  aâ‚‚â‚‚  ...  aâ‚‚â‚™]
    [ â‹®    â‹®   â‹±    â‹®  ]
    [aâ‚˜â‚  aâ‚˜â‚‚  ...  aâ‚˜â‚™]

A âˆˆ â„áµË£â¿ (mí–‰ nì—´)
```

**í…ì„œ (Tensor)**: ë‹¤ì°¨ì› ë°°ì—´
```
T âˆˆ â„áµˆÂ¹Ë£áµˆÂ²Ë£...Ë£áµˆâ¿

ì˜ˆ: [batch, seq_len, hidden_dim] = [32, 512, 768]
```

### 1.2 ë”¥ëŸ¬ë‹ì—ì„œì˜ Shape Convention

```python
# PyTorch ê´€ë¡€
batch_size = 32
seq_len = 512
hidden_dim = 768
num_heads = 12

# ì…ë ¥ í…ì„œ
x = torch.randn(batch_size, seq_len, hidden_dim)  # [B, T, D]

# Attentionì—ì„œ
Q = x @ W_Q  # [B, T, D] @ [D, D] = [B, T, D]
Q = Q.view(B, T, num_heads, head_dim)  # [B, T, H, d_k]
Q = Q.transpose(1, 2)  # [B, H, T, d_k]
```

---

## 2. í–‰ë ¬ ì—°ì‚°

### 2.1 í–‰ë ¬ ê³±ì…ˆ (Matrix Multiplication)

**ì •ì˜**:
```
C = A Ã— B ì¼ ë•Œ,
c_ij = Î£â‚– a_ik Ã— b_kj

ì¡°ê±´: A âˆˆ â„áµË£â¿, B âˆˆ â„â¿Ë£áµ– â†’ C âˆˆ â„áµË£áµ–
```

**ë”¥ëŸ¬ë‹ ì˜ˆì‹œ**:
```
ì„ í˜• ë³€í™˜: y = Wx + b
ì…ë ¥: x âˆˆ â„â¿
ê°€ì¤‘ì¹˜: W âˆˆ â„â¿Ë£áµ
ì¶œë ¥: y âˆˆ â„áµ
```

**ê³„ì‚° ë³µì¡ë„**: O(mnp)

```python
import numpy as np

# í–‰ë ¬ ê³±ì…ˆ
A = np.random.randn(3, 4)  # [3, 4]
B = np.random.randn(4, 5)  # [4, 5]
C = A @ B                   # [3, 5]

# ë˜ëŠ”
C = np.matmul(A, B)
C = np.dot(A, B)
```

### 2.2 Element-wise ì—°ì‚°

**Hadamard ê³± (âŠ™)**:
```
C = A âŠ™ B ì¼ ë•Œ,
c_ij = a_ij Ã— b_ij

ì¡°ê±´: A, Bì˜ shape ë™ì¼
```

**ë”¥ëŸ¬ë‹ í™œìš©**: Gating ë©”ì»¤ë‹ˆì¦˜ (LSTM, GRU)
```python
# Forget gate
f_t = sigmoid(W_f @ x_t)
c_t = f_t * c_{t-1}  # element-wise
```

### 2.3 Broadcasting

ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ ì—°ì‚° ì‹œ ìë™ í™•ì¥:

```python
# í¸í–¥ ë”í•˜ê¸°
x = np.random.randn(32, 512, 768)  # [B, T, D]
b = np.random.randn(768)           # [D]
y = x + b  # bê°€ [1, 1, 768]ë¡œ broadcast â†’ [32, 512, 768]
```

**Broadcasting ê·œì¹™**:
1. ì°¨ì› ìˆ˜ê°€ ë‹¤ë¥´ë©´ ì‘ì€ ìª½ ì•ì— 1 ì¶”ê°€
2. í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì€ ë‹¤ë¥¸ í…ì„œì— ë§ì¶° í™•ì¥
3. í¬ê¸°ê°€ ë‹¤ë¥´ê³  1ë„ ì•„ë‹ˆë©´ ì˜¤ë¥˜

---

## 3. ì „ì¹˜ì™€ ì—­í–‰ë ¬

### 3.1 ì „ì¹˜ (Transpose)

**ì •ì˜**: í–‰ê³¼ ì—´ êµí™˜
```
(Aáµ€)_ij = A_ji

A âˆˆ â„áµË£â¿ â†’ Aáµ€ âˆˆ â„â¿Ë£áµ
```

**ì„±ì§ˆ**:
```
(Aáµ€)áµ€ = A
(AB)áµ€ = Báµ€Aáµ€  # ìˆœì„œ ë’¤ì§‘í˜!
(A + B)áµ€ = Aáµ€ + Báµ€
```

**ë”¥ëŸ¬ë‹ í™œìš©**: Attentionì—ì„œ K^T
```python
# Attention scores
scores = Q @ K.transpose(-2, -1)  # [B, H, T, d_k] @ [B, H, d_k, T]
                                   # = [B, H, T, T]
```

### 3.2 ì—­í–‰ë ¬ (Inverse)

**ì •ì˜**: AAâ»Â¹ = Aâ»Â¹A = I (ë‹¨ìœ„í–‰ë ¬)

**ì¡°ê±´**: ì •ë°©í–‰ë ¬ì´ê³  det(A) â‰  0

```python
A = np.array([[1, 2], [3, 4]])
A_inv = np.linalg.inv(A)
print(A @ A_inv)  # [[1, 0], [0, 1]] (ë‹¨ìœ„í–‰ë ¬)
```

**ë”¥ëŸ¬ë‹ì—ì„œ**: ì§ì ‘ ì—­í–‰ë ¬ ê³„ì‚°ì€ ë“œë¬¾ (ê³„ì‚° ë¹„ìš© O(nÂ³))
- ëŒ€ì‹  gradient descent ì‚¬ìš©
- ë˜ëŠ” pseudo-inverse ì‚¬ìš©

### 3.3 ìœ ì‚¬ì—­í–‰ë ¬ (Pseudo-inverse)

**Moore-Penrose ì—­í–‰ë ¬**: Aâº

```
Aâº = (Aáµ€A)â»Â¹Aáµ€  (Aê°€ ì—´ í’€ë­í¬ì¼ ë•Œ)
```

**ìµœì†Œì œê³± í•´**:
```
Ax = b ì˜ ìµœì†Œì œê³± í•´: x = Aâºb
```

```python
A = np.random.randn(5, 3)  # ê³¼ê²°ì • ì‹œìŠ¤í…œ
A_pinv = np.linalg.pinv(A)
```

---

## 4. ë‚´ì ê³¼ ë…¸ë¦„

### 4.1 ë‚´ì  (Dot Product / Inner Product)

**ì •ì˜**:
```
a Â· b = Î£áµ¢ aáµ¢báµ¢ = aáµ€b = â€–aâ€–â€–bâ€–cos(Î¸)

ì—¬ê¸°ì„œ Î¸ëŠ” ë‘ ë²¡í„° ì‚¬ì´ì˜ ê°ë„
```

**ê¸°í•˜í•™ì  ì˜ë¯¸**: ìœ ì‚¬ë„ ì¸¡ì •
```
cos(Î¸) = (a Â· b) / (â€–aâ€–â€–bâ€–)

cos(Î¸) = 1: ê°™ì€ ë°©í–¥
cos(Î¸) = 0: ì§êµ (ë¬´ê´€)
cos(Î¸) = -1: ë°˜ëŒ€ ë°©í–¥
```

**Attentionì—ì„œì˜ í™œìš©**:
```
score(q, k) = q Â· k  # Queryì™€ Keyì˜ ìœ ì‚¬ë„
```

### 4.2 ë…¸ë¦„ (Norm)

**L1 ë…¸ë¦„ (Manhattan)**:
```
â€–xâ€–â‚ = Î£áµ¢ |xáµ¢|

í¬ì†Œì„± ìœ ë„ (Lasso regularization)
```

**L2 ë…¸ë¦„ (Euclidean)**:
```
â€–xâ€–â‚‚ = âˆš(Î£áµ¢ xáµ¢Â²)

ì¼ë°˜ì ì¸ ê±°ë¦¬, weight decay
```

**Frobenius ë…¸ë¦„ (í–‰ë ¬)**:
```
â€–Aâ€–_F = âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²) = âˆš(tr(Aáµ€A))

í–‰ë ¬ì˜ "í¬ê¸°" ì¸¡ì •
```

**âˆ-ë…¸ë¦„ (Max)**:
```
â€–xâ€–_âˆ = max_i |xáµ¢|
```

```python
x = np.array([3, -4, 5])

L1 = np.linalg.norm(x, ord=1)    # 12
L2 = np.linalg.norm(x, ord=2)    # 7.07
Linf = np.linalg.norm(x, ord=np.inf)  # 5
```

### 4.3 ì •ê·œí™” (Normalization)

**ë²¡í„° ì •ê·œí™”**:
```
x_norm = x / â€–xâ€–â‚‚

ê²°ê³¼: â€–x_normâ€–â‚‚ = 1 (ë‹¨ìœ„ ë²¡í„°)
```

**Layer Normalization**:
```python
def layer_norm(x, gamma, beta, eps=1e-5):
    """
    x: [batch, seq_len, hidden_dim]
    gamma, beta: [hidden_dim] (í•™ìŠµ íŒŒë¼ë¯¸í„°)
    """
    mean = x.mean(dim=-1, keepdim=True)
    var = x.var(dim=-1, keepdim=True)
    x_norm = (x - mean) / torch.sqrt(var + eps)
    return gamma * x_norm + beta
```

---

## 5. ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°

### 5.1 ì •ì˜

í–‰ë ¬ Aì— ëŒ€í•´:
```
Av = Î»v

v: ê³ ìœ ë²¡í„° (eigenvector) - ë°©í–¥ì´ ë³€í•˜ì§€ ì•ŠëŠ” ë²¡í„°
Î»: ê³ ìœ ê°’ (eigenvalue) - ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜
```

### 5.2 ê¸°í•˜í•™ì  ì˜ë¯¸

```
Aê°€ ë³€í™˜ í–‰ë ¬ì¼ ë•Œ:
- ê³ ìœ ë²¡í„°: ë³€í™˜ ì‹œ ë°©í–¥ì´ ë³´ì¡´ë˜ëŠ” ì¶•
- ê³ ìœ ê°’: í•´ë‹¹ ì¶• ë°©í–¥ì˜ ìŠ¤ì¼€ì¼ë§ ì •ë„
```

### 5.3 ê³„ì‚°

```python
A = np.array([[4, 2], [1, 3]])

eigenvalues, eigenvectors = np.linalg.eig(A)
# eigenvalues: [5, 2]
# eigenvectors: ê° ì—´ì´ ê³ ìœ ë²¡í„°
```

### 5.4 ë”¥ëŸ¬ë‹ì—ì„œì˜ í™œìš©

**PCA (ì°¨ì› ì¶•ì†Œ)**:
```python
# ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ ë¶„í•´
cov = X.T @ X / n
eigenvalues, eigenvectors = np.linalg.eigh(cov)

# ìƒìœ„ kê°œ ì£¼ì„±ë¶„ ì„ íƒ
top_k = eigenvectors[:, -k:]
X_reduced = X @ top_k
```

**Attention í•´ì„**:
- Attention í–‰ë ¬ì˜ ê³ ìœ ê°’ ë¶„í¬ ë¶„ì„
- ì •ë³´ ì§‘ì¤‘ë„ ì¸¡ì •

---

## 6. íŠ¹ì´ê°’ ë¶„í•´ (SVD)

### 6.1 ì •ì˜

ì„ì˜ì˜ í–‰ë ¬ A âˆˆ â„áµË£â¿ ë¶„í•´:
```
A = UÎ£Váµ€

U âˆˆ â„áµË£áµ: ì¢ŒíŠ¹ì´ë²¡í„° (ì§êµí–‰ë ¬)
Î£ âˆˆ â„áµË£â¿: íŠ¹ì´ê°’ (ëŒ€ê°í–‰ë ¬, Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0)
Váµ€ âˆˆ â„â¿Ë£â¿: ìš°íŠ¹ì´ë²¡í„° (ì§êµí–‰ë ¬)
```

### 6.2 Low-Rank ê·¼ì‚¬

ìƒìœ„ rê°œ íŠ¹ì´ê°’ë§Œ ì‚¬ìš©:
```
A â‰ˆ Aáµ£ = Î£áµ¢â‚Œâ‚Ê³ Ïƒáµ¢uáµ¢váµ¢áµ€

ì €ì¥: mÃ—r + r + rÃ—n vs ì›ë˜ mÃ—n
```

### 6.3 LoRAì™€ì˜ ì—°ê²°

**LoRA í•µì‹¬ ì•„ì´ë””ì–´**:
```
ì›ë˜ ê°€ì¤‘ì¹˜: W âˆˆ â„áµˆË£áµˆ
ì—…ë°ì´íŠ¸: Î”W = BA

B âˆˆ â„áµˆË£Ê³, A âˆˆ â„Ê³Ë£áµˆ (r << d)

íŒŒë¼ë¯¸í„°: dÂ² â†’ 2dr
ì˜ˆ: d=4096, r=16: 16M â†’ 130K (99% ê°ì†Œ)
```

```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=16):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.linear.weight.requires_grad = False  # ë™ê²°

        # LoRA íŒŒë¼ë¯¸í„°
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.scale = alpha / rank

    def forward(self, x):
        # W + Î”W = W + BA
        return self.linear(x) + (x @ self.lora_A.T @ self.lora_B.T) * self.scale
```

### 6.4 SVD ê³„ì‚°

```python
A = np.random.randn(100, 50)

U, S, Vt = np.linalg.svd(A, full_matrices=False)
# U: [100, 50]
# S: [50] (íŠ¹ì´ê°’)
# Vt: [50, 50]

# Low-rank ê·¼ì‚¬ (rank 10)
r = 10
A_approx = U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]

# ë³µì› ì˜¤ì°¨
error = np.linalg.norm(A - A_approx) / np.linalg.norm(A)
```

---

## 7. í–‰ë ¬ì˜ íŠ¹ìˆ˜ í˜•íƒœ

### 7.1 ëŒ€ì¹­ í–‰ë ¬ (Symmetric)

```
A = Aáµ€

ì„±ì§ˆ:
- ì‹¤ìˆ˜ ê³ ìœ ê°’
- ì§êµ ê³ ìœ ë²¡í„°
- ì–‘ì •ì¹˜ë©´ ëª¨ë“  ê³ ìœ ê°’ > 0
```

**ë”¥ëŸ¬ë‹**: ê³µë¶„ì‚° í–‰ë ¬, Hessian

### 7.2 ì§êµ í–‰ë ¬ (Orthogonal)

```
Qáµ€Q = QQáµ€ = I

ì„±ì§ˆ:
- Qâ»Â¹ = Qáµ€
- ê¸¸ì´ ë³´ì¡´: â€–Qxâ€– = â€–xâ€–
- ê°ë„ ë³´ì¡´
```

**ë”¥ëŸ¬ë‹**: ì§êµ ì´ˆê¸°í™”, ì•ˆì •ì ì¸ gradient flow

### 7.3 ëŒ€ê° í–‰ë ¬ (Diagonal)

```
D = diag(dâ‚, dâ‚‚, ..., dâ‚™)

ë¹„ëŒ€ê° ì›ì†Œ = 0
```

**ë”¥ëŸ¬ë‹**: ìŠ¤ì¼€ì¼ë§ (Layer Normì˜ Î³)

### 7.4 ì‚¼ê° í–‰ë ¬ (Triangular)

```
í•˜ì‚¼ê°: L (ëŒ€ê°ì„  ì•„ë˜ë§Œ 0 ì•„ë‹˜)
ìƒì‚¼ê°: U (ëŒ€ê°ì„  ìœ„ë§Œ 0 ì•„ë‹˜)
```

**ë”¥ëŸ¬ë‹**: Causal Attentionì˜ ë§ˆìŠ¤í¬

```python
# Causal mask (í•˜ì‚¼ê°)
seq_len = 10
mask = torch.tril(torch.ones(seq_len, seq_len))
# [[1, 0, 0, ...],
#  [1, 1, 0, ...],
#  [1, 1, 1, ...], ...]
```

---

## 8. í–‰ë ¬ ë¶„í•´

### 8.1 LU ë¶„í•´

```
A = LU

L: í•˜ì‚¼ê° (ëŒ€ê°ì„  1)
U: ìƒì‚¼ê°
```

ì„ í˜• ì‹œìŠ¤í…œ í’€ì´ì— í™œìš©

### 8.2 Cholesky ë¶„í•´

```
A = LLáµ€ (ì–‘ì •ì¹˜ ëŒ€ì¹­ í–‰ë ¬)

L: í•˜ì‚¼ê°
```

íš¨ìœ¨ì ì¸ ìƒ˜í”Œë§, ê³µë¶„ì‚° ì—­í–‰ë ¬

### 8.3 QR ë¶„í•´

```
A = QR

Q: ì§êµ í–‰ë ¬
R: ìƒì‚¼ê°
```

ì§êµí™”, ìµœì†Œì œê³±

---

## 9. ë”¥ëŸ¬ë‹ ì‘ìš©

### 9.1 Attention ì—°ì‚°ì˜ í–‰ë ¬ ê´€ì 

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: [batch, heads, seq_len, d_k]
    """
    d_k = Q.size(-1)

    # 1. QK^T: ìœ ì‚¬ë„ í–‰ë ¬
    # [B, H, T, d_k] @ [B, H, d_k, T] = [B, H, T, T]
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # 2. Masking (ì˜µì…˜)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))

    # 3. Softmax: í™•ë¥  ë¶„í¬
    attn_weights = F.softmax(scores, dim=-1)

    # 4. Vì™€ ê°€ì¤‘í•©
    # [B, H, T, T] @ [B, H, T, d_v] = [B, H, T, d_v]
    output = torch.matmul(attn_weights, V)

    return output, attn_weights
```

### 9.2 ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í•´ì„

```
W_Q, W_K, W_V, W_O âˆˆ â„áµˆË£áµˆ

ì—­í• :
- W_Q: ì…ë ¥ì„ "ì§ˆë¬¸" ê³µê°„ìœ¼ë¡œ íˆ¬ì˜
- W_K: ì…ë ¥ì„ "í‚¤" ê³µê°„ìœ¼ë¡œ íˆ¬ì˜
- W_V: ì…ë ¥ì„ "ê°’" ê³µê°„ìœ¼ë¡œ íˆ¬ì˜
- W_O: Multi-head ì¶œë ¥ ê²°í•©
```

### 9.3 ì´ˆê¸°í™” ì „ëµ

**Xavier/Glorot ì´ˆê¸°í™”**:
```
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))

ë˜ëŠ”

W ~ N(0, 2/(n_in + n_out))
```

**He ì´ˆê¸°í™”** (ReLUìš©):
```
W ~ N(0, 2/n_in)
```

```python
# PyTorch
nn.init.xavier_uniform_(layer.weight)
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
```

---

## 10. ìˆ˜ì¹˜ ì•ˆì •ì„±

### 10.1 ì¡°ê±´ìˆ˜ (Condition Number)

```
Îº(A) = â€–Aâ€– Ã— â€–Aâ»Â¹â€– = Ïƒ_max / Ïƒ_min

Îºê°€ í¬ë©´: ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¶ˆì•ˆì •
Îº â‰ˆ 1: ì˜ ì¡°ê±´í™”ë¨ (well-conditioned)
```

### 10.2 ìˆ˜ì¹˜ ì•ˆì •ì„± ê¸°ë²•

**Softmax ì•ˆì •í™”**:
```python
def stable_softmax(x):
    # overflow ë°©ì§€: max ë¹¼ê¸°
    x_max = x.max(dim=-1, keepdim=True).values
    exp_x = torch.exp(x - x_max)
    return exp_x / exp_x.sum(dim=-1, keepdim=True)
```

**Log-sum-exp ì•ˆì •í™”**:
```python
def stable_logsumexp(x):
    x_max = x.max(dim=-1, keepdim=True).values
    return x_max + torch.log(torch.exp(x - x_max).sum(dim=-1, keepdim=True))
```

### 10.3 Mixed Precision

```python
# FP16 + FP32 í˜¼í•©
with torch.cuda.amp.autocast():
    output = model(input)  # FP16ìœ¼ë¡œ ì—°ì‚°
    loss = criterion(output, target)

scaler.scale(loss).backward()  # gradient scaling
scaler.step(optimizer)
scaler.update()
```

---

## 11. ìš”ì•½ ì •ë¦¬í‘œ

### 11.1 ê¸°ë³¸ ì—°ì‚°

| ì—°ì‚° | í‘œê¸° | NumPy | PyTorch |
|:-----|:-----|:------|:--------|
| í–‰ë ¬ ê³± | AB | `A @ B` | `torch.matmul(A, B)` |
| ì›ì†Œë³„ ê³± | AâŠ™B | `A * B` | `A * B` |
| ì „ì¹˜ | Aáµ€ | `A.T` | `A.T` ë˜ëŠ” `A.transpose()` |
| ì—­í–‰ë ¬ | Aâ»Â¹ | `np.linalg.inv(A)` | `torch.linalg.inv(A)` |
| ë‚´ì  | aÂ·b | `np.dot(a, b)` | `torch.dot(a, b)` |

### 11.2 ë¶„í•´

| ë¶„í•´ | ìš©ë„ | í•¨ìˆ˜ |
|:-----|:-----|:-----|
| ê³ ìœ ê°’ | PCA, ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ | `np.linalg.eig()` |
| SVD | LoRA, ì••ì¶• | `np.linalg.svd()` |
| QR | ì§êµí™” | `np.linalg.qr()` |
| Cholesky | ì–‘ì •ì¹˜ ë¶„í•´ | `np.linalg.cholesky()` |

### 11.3 Shape ë³€í™˜ (PyTorch)

| í•¨ìˆ˜ | ìš©ë„ |
|:-----|:-----|
| `view()` | í˜•íƒœ ë³€ê²½ (ì—°ì† ë©”ëª¨ë¦¬) |
| `reshape()` | í˜•íƒœ ë³€ê²½ (ìë™ ë³µì‚¬) |
| `transpose()` | ë‘ ì°¨ì› êµí™˜ |
| `permute()` | ì°¨ì› ìˆœì„œ ë³€ê²½ |
| `squeeze()` | í¬ê¸° 1ì¸ ì°¨ì› ì œê±° |
| `unsqueeze()` | ì°¨ì› ì¶”ê°€ |

---

> ğŸ’¡ **ë³¸ë¬¸ ì—°ê²°**
> - [1.1 ìˆ˜í•™ì  ê¸°ì´ˆ](../../01_ë”¥ëŸ¬ë‹_Transformer_ê¸°ì´ˆ/01_ìˆ˜í•™ì _ê¸°ì´ˆ.md)
> - [ë¶€ë¡ A0: ìˆ˜í•™ ê¸°í˜¸ ì‚¬ì „](A0_ìˆ˜í•™_ê¸°í˜¸_ì‚¬ì „.md)
> - [ë¶€ë¡ B1: Attention ìˆ˜ì‹ ìœ ë„](../B_ì¦ëª…_ìœ ë„/B1_Attention_ìˆ˜ì‹_ìœ ë„.md)

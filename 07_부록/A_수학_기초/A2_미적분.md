# A2. ë¯¸ì ë¶„ (Calculus)

> ë”¥ëŸ¬ë‹ í•™ìŠµì˜ í•µì‹¬ì¸ Backpropagation ì´í•´ë¥¼ ìœ„í•œ ë¯¸ì ë¶„ ê¸°ì´ˆ

---

## 1. ë¯¸ë¶„ì˜ ê¸°ë³¸ ê°œë…

### 1.1 ë¯¸ë¶„ì˜ ì •ì˜

**ê¸°í•˜í•™ì  ì˜ë¯¸**: í•¨ìˆ˜ ê·¸ë˜í”„ì˜ ì ‘ì„  ê¸°ìš¸ê¸°

```
f'(x) = lim(hâ†’0) [f(x+h) - f(x)] / h
```

**ë”¥ëŸ¬ë‹ì—ì„œì˜ ì˜ë¯¸**: íŒŒë¼ë¯¸í„° ë³€í™”ê°€ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

```
Wê°€ ì¡°ê¸ˆ ë³€í•  ë•Œ, ì†ì‹¤ Lì´ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ê°€?
â†’ âˆ‚L/âˆ‚W (ì´ ê°’ìœ¼ë¡œ W ì—…ë°ì´íŠ¸ ë°©í–¥ ê²°ì •)
```

### 1.2 ê¸°ë³¸ ë¯¸ë¶„ ê³µì‹

| í•¨ìˆ˜ f(x) | ë„í•¨ìˆ˜ f'(x) | ë”¥ëŸ¬ë‹ í™œìš© |
|:----------|:-------------|:------------|
| xâ¿ | nÂ·xâ¿â»Â¹ | ë‹¤í•­ì‹ |
| eË£ | eË£ | Softmax, ì •ê·œí™” |
| ln(x) | 1/x | Cross-Entropy |
| sin(x) | cos(x) | Positional Encoding |
| cos(x) | -sin(x) | Positional Encoding |

### 1.3 ë¯¸ë¶„ ë²•ì¹™

**í•©ì˜ ë²•ì¹™**
```
(f + g)' = f' + g'
```

**ê³±ì˜ ë²•ì¹™**
```
(f Â· g)' = f' Â· g + f Â· g'

ì˜ˆ: (x Â· eË£)' = 1 Â· eË£ + x Â· eË£ = eË£(1 + x)
```

**ëª«ì˜ ë²•ì¹™**
```
(f/g)' = (f'Â·g - fÂ·g') / gÂ²
```

---

## 2. Chain Rule (ì—°ì‡„ ë²•ì¹™)

### 2.1 ê°œë…

í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ ë²•ì¹™. **Backpropagationì˜ ìˆ˜í•™ì  ê¸°ë°˜**.

```
y = f(g(x)) ì¼ ë•Œ,
dy/dx = (dy/du) Â· (du/dx)

ì—¬ê¸°ì„œ u = g(x)
```

**ì§ê´€ì  ì´í•´**:
```
x â†’ g â†’ u â†’ f â†’ y

xê°€ uì— ë¯¸ì¹˜ëŠ” ì˜í–¥ Ã— uê°€ yì— ë¯¸ì¹˜ëŠ” ì˜í–¥ = xê°€ yì— ë¯¸ì¹˜ëŠ” ì˜í–¥
```

### 2.2 ë‹¨ì¼ ë³€ìˆ˜ Chain Rule

**ì˜ˆì œ 1: ê°„ë‹¨í•œ í•©ì„± í•¨ìˆ˜**
```
y = (3x + 2)Â²

ì„¤ì •: u = 3x + 2, y = uÂ²

ê³„ì‚°:
- du/dx = 3
- dy/du = 2u = 2(3x + 2)

ê²°ê³¼:
dy/dx = dy/du Ã— du/dx = 2(3x + 2) Ã— 3 = 6(3x + 2)
```

**ì˜ˆì œ 2: Sigmoid í•¨ìˆ˜**
```
Ïƒ(x) = 1 / (1 + eâ»Ë£)

ì„¤ì •: u = 1 + eâ»Ë£, Ïƒ = 1/u = uâ»Â¹

ê³„ì‚°:
- du/dx = -eâ»Ë£
- dÏƒ/du = -uâ»Â² = -1/(1 + eâ»Ë£)Â²

ê²°ê³¼:
dÏƒ/dx = dÏƒ/du Ã— du/dx
      = -1/(1 + eâ»Ë£)Â² Ã— (-eâ»Ë£)
      = eâ»Ë£ / (1 + eâ»Ë£)Â²

ì •ë¦¬: Ïƒ'(x) = Ïƒ(x) Â· (1 - Ïƒ(x))  â† ë§¤ìš° ì¤‘ìš”!
```

### 2.3 ë‹¤ë³€ìˆ˜ Chain Rule

ì—¬ëŸ¬ ê²½ë¡œë¡œ ì˜í–¥ì´ ì „íŒŒë  ë•Œ:

```
z = f(x, y), x = g(t), y = h(t)

dz/dt = (âˆ‚z/âˆ‚x)(dx/dt) + (âˆ‚z/âˆ‚y)(dy/dt)
```

**ë”¥ëŸ¬ë‹ì—ì„œì˜ ì˜ë¯¸**:
```
Lì´ ì—¬ëŸ¬ ê²½ë¡œë¡œ Wì— ì˜ì¡´í•  ë•Œ, ëª¨ë“  ê²½ë¡œì˜ gradientë¥¼ í•©ì‚°
```

---

## 3. í¸ë¯¸ë¶„ (Partial Derivative)

### 3.1 ê°œë…

ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì—ì„œ í•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•´ì„œë§Œ ë¯¸ë¶„ (ë‚˜ë¨¸ì§€ëŠ” ìƒìˆ˜ ì·¨ê¸‰)

```
f(x, y) = xÂ²y + 3xyÂ²

âˆ‚f/âˆ‚x = 2xy + 3yÂ²  (yëŠ” ìƒìˆ˜ ì·¨ê¸‰)
âˆ‚f/âˆ‚y = xÂ² + 6xy   (xëŠ” ìƒìˆ˜ ì·¨ê¸‰)
```

### 3.2 Gradient (ê¸°ìš¸ê¸° ë²¡í„°)

ëª¨ë“  í¸ë¯¸ë¶„ì„ ë²¡í„°ë¡œ ëª¨ì€ ê²ƒ:

```
âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]

âˆ‡f(x,y) = [2xy + 3yÂ², xÂ² + 6xy]
```

**ê¸°í•˜í•™ì  ì˜ë¯¸**: í•¨ìˆ˜ê°€ ê°€ì¥ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥

**Gradient Descent**: ë°˜ëŒ€ ë°©í–¥(âˆ’âˆ‡f)ìœ¼ë¡œ ì´ë™í•˜ì—¬ ìµœì†Ÿê°’ íƒìƒ‰

```python
# Gradient Descent ê¸°ë³¸ í˜•íƒœ
W = W - learning_rate * gradient
```

---

## 4. Neural Networkì—ì„œì˜ Chain Rule

### 4.1 ë‹¨ì¼ ë‰´ëŸ°

```
ì…ë ¥: x
ê°€ì¤‘ì¹˜: W
í¸í–¥: b
ì¶œë ¥: a = Ïƒ(Wx + b)
ì†ì‹¤: L

Forward:
z = Wx + b
a = Ïƒ(z)

Backward (âˆ‚L/âˆ‚W ê³„ì‚°):
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚W

ê° í•­:
- âˆ‚L/âˆ‚a: ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ê³„ì‚° (ë‹¤ìŒ ì¸µì—ì„œ ì „íŒŒ)
- âˆ‚a/âˆ‚z: í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„ = Ïƒ'(z)
- âˆ‚z/âˆ‚W: ì„ í˜• ë³€í™˜ì˜ ë¯¸ë¶„ = x
```

### 4.2 2ì¸µ Neural Network

```
Architecture:
x â†’ [Wâ‚, bâ‚] â†’ zâ‚ â†’ [ReLU] â†’ aâ‚ â†’ [Wâ‚‚, bâ‚‚] â†’ zâ‚‚ â†’ [softmax] â†’ Å· â†’ L

Forward:
zâ‚ = Wâ‚x + bâ‚
aâ‚ = ReLU(zâ‚)
zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
Å· = softmax(zâ‚‚)
L = CrossEntropy(y, Å·)

Backward (âˆ‚L/âˆ‚Wâ‚):
âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚Å· Ã— âˆ‚Å·/âˆ‚zâ‚‚ Ã— âˆ‚zâ‚‚/âˆ‚aâ‚ Ã— âˆ‚aâ‚/âˆ‚zâ‚ Ã— âˆ‚zâ‚/âˆ‚Wâ‚

ë‹¨ê³„ë³„:
1. âˆ‚L/âˆ‚Å· = d_CrossEntropy     # ì†ì‹¤ í•¨ìˆ˜ ë¯¸ë¶„
2. âˆ‚Å·/âˆ‚zâ‚‚ = d_softmax         # softmax ë¯¸ë¶„
3. âˆ‚zâ‚‚/âˆ‚aâ‚ = Wâ‚‚               # ì„ í˜•: zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
4. âˆ‚aâ‚/âˆ‚zâ‚ = d_ReLU(zâ‚)       # ReLU ë¯¸ë¶„: zâ‚ > 0 ? 1 : 0
5. âˆ‚zâ‚/âˆ‚Wâ‚ = x                # ì„ í˜•: zâ‚ = Wâ‚x + bâ‚
```

### 4.3 Python êµ¬í˜„

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)  # Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Forward + Backward ì˜ˆì‹œ
class SimpleLayer:
    def __init__(self, input_dim, output_dim):
        self.W = np.random.randn(input_dim, output_dim) * 0.01
        self.b = np.zeros(output_dim)

    def forward(self, x):
        self.x = x  # ì €ì¥ (backwardì—ì„œ ì‚¬ìš©)
        self.z = x @ self.W + self.b
        return self.z

    def backward(self, dL_dz):
        # Chain Rule ì ìš©
        dL_dW = self.x.T @ dL_dz  # âˆ‚L/âˆ‚W = xáµ€ Ã— âˆ‚L/âˆ‚z
        dL_db = np.sum(dL_dz, axis=0)  # âˆ‚L/âˆ‚b = Î£ âˆ‚L/âˆ‚z
        dL_dx = dL_dz @ self.W.T  # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Ã— Wáµ€

        self.dW = dL_dW
        self.db = dL_db
        return dL_dx
```

---

## 5. í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„

### 5.1 Sigmoid

```
Ïƒ(x) = 1 / (1 + eâ»Ë£)
Ïƒ'(x) = Ïƒ(x) Â· (1 - Ïƒ(x))

íŠ¹ì§•:
- ì¶œë ¥ ë²”ìœ„: (0, 1)
- ìµœëŒ€ ê¸°ìš¸ê¸°: 0.25 (x=0ì¼ ë•Œ)
- ë¬¸ì œì : Vanishing Gradient (ì–‘ ëì—ì„œ ê¸°ìš¸ê¸° â‰ˆ 0)
```

### 5.2 Tanh

```
tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)
tanh'(x) = 1 - tanhÂ²(x)

íŠ¹ì§•:
- ì¶œë ¥ ë²”ìœ„: (-1, 1)
- Zero-centered
- ìµœëŒ€ ê¸°ìš¸ê¸°: 1 (x=0ì¼ ë•Œ)
```

### 5.3 ReLU

```
ReLU(x) = max(0, x)

ReLU'(x) = {
    1  if x > 0
    0  if x < 0
    undefined if x = 0 (ì‹¤ë¬´: 0 ë˜ëŠ” 1)
}

íŠ¹ì§•:
- ê³„ì‚° íš¨ìœ¨ì 
- Vanishing Gradient í•´ê²°
- ë¬¸ì œì : Dead ReLU (x < 0ì´ë©´ ì˜êµ¬ì ìœ¼ë¡œ 0)
```

### 5.4 Leaky ReLU

```
LeakyReLU(x) = {
    x        if x > 0
    Î±Â·x      if x â‰¤ 0  (Î± = 0.01 ë“± ì‘ì€ ê°’)
}

LeakyReLU'(x) = {
    1  if x > 0
    Î±  if x â‰¤ 0
}

íŠ¹ì§•:
- Dead ReLU ë¬¸ì œ í•´ê²°
```

### 5.5 GELU (Gaussian Error Linear Unit)

```
GELU(x) = x Â· Î¦(x)
        â‰ˆ 0.5x(1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³)))

ì—¬ê¸°ì„œ Î¦(x)ëŠ” í‘œì¤€ì •ê·œë¶„í¬ì˜ CDF

íŠ¹ì§•:
- í˜„ëŒ€ Transformerì˜ í‘œì¤€ í™œì„±í™” í•¨ìˆ˜
- ë¶€ë“œëŸ¬ìš´ ê³¡ì„  (ë¯¸ë¶„ ê°€ëŠ¥)
- GPT, BERT, ViT ë“±ì—ì„œ ì‚¬ìš©
```

---

## 6. Softmaxì™€ Cross-Entropy

### 6.1 Softmax í•¨ìˆ˜

```
softmax(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)

íŠ¹ì§•:
- ì¶œë ¥ í•© = 1 (í™•ë¥  ë¶„í¬)
- ëª¨ë“  ì¶œë ¥ > 0
```

### 6.2 Softmaxì˜ ë¯¸ë¶„

```
âˆ‚softmax_i/âˆ‚x_j = {
    sáµ¢(1 - sáµ¢)    if i = j
    -sáµ¢ Â· sâ±¼      if i â‰  j
}

ì—¬ê¸°ì„œ sáµ¢ = softmax(x)áµ¢

Jacobian í–‰ë ¬:
J = diag(s) - s Â· sáµ€
```

### 6.3 Cross-Entropy ì†ì‹¤

```
L = -Î£áµ¢ yáµ¢ log(Å·áµ¢)

One-hot yì¼ ë•Œ (ì •ë‹µ í´ë˜ìŠ¤ = c):
L = -log(Å·c)
```

### 6.4 Softmax + Cross-Entropyì˜ ë¯¸ë¶„

**í•µì‹¬ ê²°ê³¼**:
```
âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢

ì¦‰, ì˜ˆì¸¡ - ì •ë‹µ
```

**ìœ ë„**:
```
L = -Î£áµ¢ yáµ¢ log(softmax(záµ¢))

âˆ‚L/âˆ‚zâ±¼ = -Î£áµ¢ yáµ¢ Â· (1/softmax(záµ¢)) Â· âˆ‚softmax(záµ¢)/âˆ‚zâ±¼

ì •ë¦¬í•˜ë©´:
âˆ‚L/âˆ‚zâ±¼ = softmax(zâ±¼) - yâ±¼ = Å·â±¼ - yâ±¼
```

**êµ¬í˜„ì´ ê°„ë‹¨í•œ ì´ìœ **: ë³µì¡í•œ Jacobian ê³„ì‚° ì—†ì´ ë°”ë¡œ `Å· - y`

```python
def softmax_cross_entropy_backward(logits, labels):
    # labels: one-hot encoded
    probs = softmax(logits)
    return probs - labels  # ë§¤ìš° ê°„ë‹¨!
```

---

## 7. Jacobianê³¼ Hessian

### 7.1 Jacobian í–‰ë ¬

ë²¡í„° í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ í–‰ë ¬ë¡œ ì •ë¦¬:

```
f: â„â¿ â†’ â„áµ ì¼ ë•Œ,

J = [âˆ‚fâ‚/âˆ‚xâ‚  âˆ‚fâ‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚/âˆ‚xâ‚™]
    [âˆ‚fâ‚‚/âˆ‚xâ‚  âˆ‚fâ‚‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚‚/âˆ‚xâ‚™]
    [   ...      ...    ...    ...   ]
    [âˆ‚fâ‚˜/âˆ‚xâ‚  âˆ‚fâ‚˜/âˆ‚xâ‚‚  ...  âˆ‚fâ‚˜/âˆ‚xâ‚™]

í¬ê¸°: m Ã— n
```

**ë”¥ëŸ¬ë‹ì—ì„œì˜ ìš©ë„**: ì¸µ ì‚¬ì´ì˜ gradient ì „íŒŒ

```
âˆ‚L/âˆ‚x = Jáµ€ Ã— âˆ‚L/âˆ‚f
```

### 7.2 Hessian í–‰ë ¬

2ì°¨ í¸ë¯¸ë¶„ì˜ í–‰ë ¬:

```
H = [âˆ‚Â²f/âˆ‚xâ‚Â²    âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ...]
    [âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚Â²    ...]
    [   ...          ...      ...]

í¬ê¸°: n Ã— n (ëŒ€ì¹­ í–‰ë ¬)
```

**ë”¥ëŸ¬ë‹ì—ì„œì˜ ìš©ë„**:
- ê³¡ë¥  ì •ë³´ (ìµœì í™” ì†ë„)
- 2ì°¨ ìµœì í™” ë°©ë²• (Newton's method)
- ê³„ì‚° ë¹„ìš©ì´ ë†’ì•„ ì‹¤ë¬´ì—ì„  ê·¼ì‚¬ ì‚¬ìš©

---

## 8. Taylor ì „ê°œ

### 8.1 1ì°¨ ê·¼ì‚¬ (ì„ í˜• ê·¼ì‚¬)

```
f(x + Î”x) â‰ˆ f(x) + f'(x)Â·Î”x

ë‹¤ë³€ìˆ˜:
f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Â·Î”x
```

**Gradient Descentì˜ ê¸°ë°˜**:
```
L(W - Î·âˆ‡L) â‰ˆ L(W) - Î·Â·â€–âˆ‡Lâ€–Â²

Î·ê°€ ì‘ìœ¼ë©´ ì†ì‹¤ ê°ì†Œ ë³´ì¥
```

### 8.2 2ì°¨ ê·¼ì‚¬

```
f(x + Î”x) â‰ˆ f(x) + f'(x)Â·Î”x + (1/2)Â·f''(x)Â·Î”xÂ²

ë‹¤ë³€ìˆ˜:
f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€Â·Î”x + (1/2)Â·Î”xáµ€Â·HÂ·Î”x
```

**Newton's Method**:
```
Î”x* = -Hâ»Â¹Â·âˆ‡f

ìµœì ì˜ ìŠ¤í… (but H ê³„ì‚° ë¹„ìš© ë†’ìŒ)
```

---

## 9. ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ Gradient Checking

### 9.1 ìˆ˜ì¹˜ ë¯¸ë¶„ (Numerical Gradient)

```
f'(x) â‰ˆ [f(x + Îµ) - f(x - Îµ)] / (2Îµ)

Îµ â‰ˆ 1e-5 ~ 1e-7
```

**ì¤‘ì•™ ì°¨ë¶„ì´ ë” ì •í™•í•œ ì´ìœ **:
- ì „ë°© ì°¨ë¶„: O(Îµ) ì˜¤ì°¨
- ì¤‘ì•™ ì°¨ë¶„: O(ÎµÂ²) ì˜¤ì°¨

### 9.2 Gradient Checking

Backprop êµ¬í˜„ ê²€ì¦:

```python
def gradient_check(f, x, analytic_grad, epsilon=1e-5):
    """
    ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ í•´ì„ì  ë¯¸ë¶„ ë¹„êµ
    """
    numerical_grad = np.zeros_like(x)

    for i in range(len(x)):
        x_plus = x.copy()
        x_plus[i] += epsilon
        x_minus = x.copy()
        x_minus[i] -= epsilon

        numerical_grad[i] = (f(x_plus) - f(x_minus)) / (2 * epsilon)

    # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°
    diff = np.linalg.norm(numerical_grad - analytic_grad)
    norm = np.linalg.norm(numerical_grad) + np.linalg.norm(analytic_grad)
    relative_error = diff / (norm + 1e-8)

    print(f"Relative error: {relative_error}")
    return relative_error < 1e-5  # Trueë©´ êµ¬í˜„ ì •í™•
```

---

## 10. ìš”ì•½: Backpropagationì˜ í•µì‹¬

### 10.1 Forward Pass

```
ì…ë ¥ â†’ ì¸µ1 â†’ ì¸µ2 â†’ ... â†’ ì¶œë ¥ â†’ ì†ì‹¤
    ì €ì¥     ì €ì¥          ì €ì¥
```

### 10.2 Backward Pass

```
ì†ì‹¤ â† ì¸µn â† ì¸µn-1 â† ... â† ì¸µ1
  âˆ‚L/âˆ‚z   âˆ‚L/âˆ‚a      âˆ‚L/âˆ‚W (Chain Rule)
```

### 10.3 ì—…ë°ì´íŠ¸

```
W â† W - Î· Â· âˆ‚L/âˆ‚W
```

### 10.4 ì „ì²´ íë¦„ ìš”ì•½

| ë‹¨ê³„ | ìˆ˜í•™ | ì½”ë“œ |
|:-----|:-----|:-----|
| Forward | z = Wx + b | `z = x @ W + b` |
| Activation | a = Ïƒ(z) | `a = relu(z)` |
| Loss | L = loss(Å·, y) | `L = cross_entropy(y_hat, y)` |
| Backward | âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Ã— xáµ€ | `dW = x.T @ dz` |
| Update | W â† W - Î·âˆ‡W | `W -= lr * dW` |

---

> ğŸ’¡ **ë³¸ë¬¸ ì—°ê²°**
> - [1.1 ìˆ˜í•™ì  ê¸°ì´ˆ](../../01_ë”¥ëŸ¬ë‹_Transformer_ê¸°ì´ˆ/01_ìˆ˜í•™ì _ê¸°ì´ˆ.md)
> - [ë¶€ë¡ B2: Backpropagation ìœ ë„](../B_ì¦ëª…_ìœ ë„/B2_Backpropagation_ìœ ë„.md)
> - [ë¶€ë¡ A0: ìˆ˜í•™ ê¸°í˜¸ ì‚¬ì „](A0_ìˆ˜í•™_ê¸°í˜¸_ì‚¬ì „.md)

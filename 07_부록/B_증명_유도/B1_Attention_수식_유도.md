# B1. Attention ìˆ˜ì‹ ìœ ë„

> Transformerì˜ í•µì‹¬ì¸ Self-Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™ì  ìœ ë„ì™€ í•´ì„

---

## 1. Attentionì˜ ì§ê´€ì  ì´í•´

### 1.1 ì™œ Attentionì´ í•„ìš”í•œê°€?

**ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ (RNN)**:
```
ì‹œí€€ìŠ¤: [ë‹¨ì–´1, ë‹¨ì–´2, ..., ë‹¨ì–´T]

RNN: ìˆœì°¨ ì²˜ë¦¬ â†’ ë³‘ëª©
- ë¨¼ ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ ì–´ë ¤ì›€
- ë³‘ë ¬í™” ë¶ˆê°€
```

**Attentionì˜ í•´ê²°ì±…**:
```
ëª¨ë“  ìœ„ì¹˜ë¥¼ ì§ì ‘ ì—°ê²°
- ê±°ë¦¬ì™€ ë¬´ê´€í•˜ê²Œ ì •ë³´ ì ‘ê·¼
- ì™„ì „ ë³‘ë ¬í™” ê°€ëŠ¥
```

### 1.2 Attentionì˜ í•µì‹¬ ì§ˆë¬¸

```
"ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í•  ê²ƒì¸ê°€?"

Query (Q): ë‚´ê°€ ì°¾ëŠ” ê²ƒ - "ë¬´ì—‡ì„ ì›í•˜ëŠ”ê°€?"
Key (K): ì •ë³´ì˜ ì¸ë±ìŠ¤ - "ê° ìœ„ì¹˜ê°€ ì–´ë–¤ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ”ê°€?"
Value (V): ì‹¤ì œ ì •ë³´ - "í•´ë‹¹ ìœ„ì¹˜ì˜ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?"
```

---

## 2. Scaled Dot-Product Attention ìœ ë„

### 2.1 ìˆ˜ì‹

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

### 2.2 ë‹¨ê³„ë³„ ìœ ë„

#### Step 1: ìœ ì‚¬ë„ ì¸¡ì •

**ëª©í‘œ**: Queryì™€ ê° Key ê°„ì˜ ê´€ë ¨ì„± ì¸¡ì •

**ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„**:
```
score(q, k) = q Â· k = Î£áµ¢ qáµ¢káµ¢

ê¸°í•˜í•™ì  ì˜ë¯¸:
q Â· k = â€–qâ€– Ã— â€–kâ€– Ã— cos(Î¸)

â†’ ë°©í–¥ì´ ë¹„ìŠ·í• ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
```

**í–‰ë ¬ í˜•íƒœ**:
```
Q âˆˆ â„áµ€Ë£áµˆáµ (Tê°œì˜ query, ê° d_k ì°¨ì›)
K âˆˆ â„áµ€Ë£áµˆáµ (Tê°œì˜ key, ê° d_k ì°¨ì›)

Scores = QK^T âˆˆ â„áµ€Ë£áµ€

scores[i,j] = Query_iì™€ Key_jì˜ ìœ ì‚¬ë„
```

#### Step 2: Scaling (âˆšd_kë¡œ ë‚˜ëˆ„ê¸°)

**ë¬¸ì œ ìƒí™©**:
```
q, kì˜ ê° ì„±ë¶„ì´ í‰ê·  0, ë¶„ì‚° 1ì¸ ë…ë¦½ í™•ë¥ ë³€ìˆ˜ë¼ ê°€ì •

q Â· k = Î£áµ¢ qáµ¢káµ¢

ê¸°ëŒ“ê°’: E[q Â· k] = Î£áµ¢ E[qáµ¢]E[káµ¢] = 0
ë¶„ì‚°: Var(q Â· k) = Î£áµ¢ Var(qáµ¢káµ¢) = d_k Ã— 1 = d_k
```

**d_kê°€ ì»¤ì§€ë©´**:
```
d_k = 64ì¼ ë•Œ, ë‚´ì ì˜ í‘œì¤€í¸ì°¨ = âˆš64 = 8
d_k = 512ì¼ ë•Œ, ë‚´ì ì˜ í‘œì¤€í¸ì°¨ = âˆš512 â‰ˆ 22.6

â†’ ê°’ì´ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆìŒ
```

**Softmax ë¬¸ì œ**:
```
softmax(x)_i = exp(x_i) / Î£â±¼ exp(x_j)

x ê°’ì´ í¬ë©´:
- ê°€ì¥ í° ê°’ì— ê±°ì˜ ëª¨ë“  í™•ë¥  ì§‘ì¤‘
- gradientê°€ ê±°ì˜ 0 (saturation)
```

**í•´ê²°ì±…: Scaling**:
```
scaled_scores = QK^T / âˆšd_k

ë¶„ì‚°: Var(scaled_scores) = d_k / d_k = 1

â†’ softmax ì…ë ¥ì˜ ë¶„ì‚°ì„ 1ë¡œ ì •ê·œí™”
```

#### Step 3: Softmax ì ìš©

**í™•ë¥  ë¶„í¬ ë³€í™˜**:
```
attention_weights = softmax(scaled_scores)

ì„±ì§ˆ:
- ëª¨ë“  ê°’ â‰¥ 0
- ê° í–‰ì˜ í•© = 1
- ë†’ì€ score â†’ ë†’ì€ ê°€ì¤‘ì¹˜
```

**ìˆ˜ì‹**:
```
attention_weights[i,j] = exp(score[i,j]) / Î£â‚– exp(score[i,k])

ì˜ë¯¸: Query_iê°€ Key_jì— ë¶€ì—¬í•˜ëŠ” attention ê°€ì¤‘ì¹˜
```

#### Step 4: Value ê°€ì¤‘í•©

**ìµœì¢… ì¶œë ¥**:
```
Output = attention_weights Ã— V

output[i] = Î£â±¼ attention_weights[i,j] Ã— V[j]

ì˜ë¯¸: Query_iì˜ ì¶œë ¥ì€ ëª¨ë“  Valueì˜ ê°€ì¤‘í•©
(attention_weightsê°€ ê°€ì¤‘ì¹˜ ì—­í• )
```

### 2.3 ì „ì²´ ìˆ˜ì‹ ì •ë¦¬

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

Shape ì¶”ì :
Q: [B, T_q, d_k]
K: [B, T_k, d_k]
V: [B, T_k, d_v]

QK^T: [B, T_q, d_k] Ã— [B, d_k, T_k] = [B, T_q, T_k]
softmax: [B, T_q, T_k]
Output: [B, T_q, T_k] Ã— [B, T_k, d_v] = [B, T_q, d_v]
```

---

## 3. Multi-Head Attention ìœ ë„

### 3.1 ë™ê¸°

**ë‹¨ì¼ Attentionì˜ í•œê³„**:
```
í•˜ë‚˜ì˜ attentionë§Œìœ¼ë¡œëŠ” ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— í¬ì°©í•˜ê¸° ì–´ë ¤ì›€
- ë¬¸ë²•ì  ê´€ê³„ (ì£¼ì–´-ë™ì‚¬)
- ì˜ë¯¸ì  ê´€ê³„ (ë™ì˜ì–´, ë°˜ì˜ì–´)
- ìœ„ì¹˜ì  ê´€ê³„ (ì¸ì ‘ ë‹¨ì–´)
```

**í•´ê²°ì±…: Multiple Heads**:
```
ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ attentionì„ ë³‘ë ¬ ìˆ˜í–‰
ê° headê°€ ë‹¤ë¥¸ ê´€ê³„ì— ì§‘ì¤‘
```

### 3.2 ìˆ˜ì‹ ìœ ë„

**ì…ë ¥ íˆ¬ì˜**:
```
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

W_i^Q âˆˆ â„áµˆáµáµ’áµˆáµ‰Ë¡ Ë£ áµˆáµ
W_i^K âˆˆ â„áµˆáµáµ’áµˆáµ‰Ë¡ Ë£ áµˆáµ
W_i^V âˆˆ â„áµˆáµáµ’áµˆáµ‰Ë¡ Ë£ áµˆáµ›

ê° headëŠ” ë‹¤ë¥¸ íˆ¬ì˜ì„ í•™ìŠµ
```

**Head ê²°í•©**:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Ã— W^O

Concat: [B, T, h Ã— d_v]
W^O âˆˆ â„â½Ê°Ë£áµˆáµ›â¾ Ë£ áµˆáµáµ’áµˆáµ‰Ë¡
Output: [B, T, d_model]
```

### 3.3 ì°¨ì› ì„¤ê³„

**ì¼ë°˜ì  ì„¤ì •**:
```
d_model = 768 (ì˜ˆ: BERT-base)
h = 12 (head ìˆ˜)
d_k = d_v = d_model / h = 64
```

**íŒŒë¼ë¯¸í„° ìˆ˜**:
```
ë‹¨ì¼ head: d_model Ã— d_model Ã— 4 (Q, K, V, O)
Multi-head: d_model Ã— d_k Ã— h Ã— 3 + d_model Ã— d_model
         = d_model Ã— d_model Ã— 4 (ë™ì¼!)

â†’ íŒŒë¼ë¯¸í„° ìˆ˜ ë™ì¼í•˜ë©´ì„œ í‘œí˜„ë ¥ ì¦ê°€
```

### 3.4 êµ¬í˜„

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # 1. ì„ í˜• íˆ¬ì˜
        Q = self.W_Q(Q)  # [B, T, d_model]
        K = self.W_K(K)
        V = self.W_V(V)

        # 2. Head ë¶„í• 
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # [B, H, T, d_k]

        # 3. Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # [B, H, T, T]

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)  # [B, H, T, d_k]

        # 4. Head ê²°í•©
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # [B, T, d_model]

        # 5. ì¶œë ¥ íˆ¬ì˜
        output = self.W_O(output)

        return output, attn_weights
```

---

## 4. Self-Attention vs Cross-Attention

### 4.1 Self-Attention

```
Q, K, V ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ ìœ ë„

X â†’ W_Q â†’ Q
X â†’ W_K â†’ K
X â†’ W_V â†’ V

ìš©ë„: ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ê´€ê³„ í•™ìŠµ
ì˜ˆ: "The cat sat on the mat" â†’ "cat"ê³¼ "sat"ì˜ ê´€ê³„
```

### 4.2 Cross-Attention

```
QëŠ” í•œ ì‹œí€€ìŠ¤, Kì™€ VëŠ” ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì—ì„œ ìœ ë„

Decoder X â†’ W_Q â†’ Q
Encoder Y â†’ W_K â†’ K
Encoder Y â†’ W_V â†’ V

ìš©ë„: ë‘ ì‹œí€€ìŠ¤ ê°„ì˜ ê´€ê³„ í•™ìŠµ
ì˜ˆ: ë²ˆì—­ì—ì„œ sourceì™€ target ì—°ê²°
```

### 4.3 VLMì—ì„œì˜ Cross-Attention

```
í…ìŠ¤íŠ¸ í† í°ì´ ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ì°¸ì¡°:

Text â†’ W_Q â†’ Q        # "ì´ ì‚¬ì§„ì—ì„œ"
Image â†’ W_K â†’ K       # ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤
Image â†’ W_V â†’ V

â†’ í…ìŠ¤íŠ¸ê°€ ê´€ë ¨ ì´ë¯¸ì§€ ì˜ì—­ì— ì§‘ì¤‘
```

---

## 5. Causal (Masked) Attention ìœ ë„

### 5.1 í•„ìš”ì„±

**ìê¸°íšŒê·€ ìƒì„±**:
```
"I love" â†’ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡

ì¡°ê±´: ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ
- "love"ë¥¼ ì˜ˆì¸¡í•  ë•Œ "I"ë§Œ ì°¸ì¡°
- ë¯¸ë˜ í† í° ì°¸ì¡° ê¸ˆì§€ (ì •ë³´ ëˆ„ì¶œ ë°©ì§€)
```

### 5.2 Causal Mask

```
T = 4ì¼ ë•Œ mask:

     [1]  í† í°1ì€ í† í°1ë§Œ ë´„
     [2]  í† í°2ëŠ” í† í°1,2 ë´„
     [3]  í† í°3ì€ í† í°1,2,3 ë´„
     [4]  í† í°4ëŠ” ëª¨ë‘ ë´„

    1  2  3  4
1 [ 1  0  0  0 ]
2 [ 1  1  0  0 ]
3 [ 1  1  1  0 ]
4 [ 1  1  1  1 ]

= í•˜ì‚¼ê° í–‰ë ¬ (lower triangular)
```

### 5.3 ë§ˆìŠ¤í‚¹ ì ìš©

```
scores = QK^T / âˆšd_k              # [T, T]
scores = scores.masked_fill(mask == 0, -inf)

softmax í›„:
- -inf â†’ exp(-inf) = 0
- ë¯¸ë˜ í† í°ì— ëŒ€í•œ attention = 0
```

### 5.4 êµ¬í˜„

```python
def causal_attention(Q, K, V):
    """
    Q, K, V: [B, H, T, d_k]
    """
    T = Q.size(2)

    # Causal mask ìƒì„±
    mask = torch.tril(torch.ones(T, T, device=Q.device))

    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
    scores = scores.masked_fill(mask == 0, float('-inf'))

    attn_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, V)

    return output
```

---

## 6. Attentionì˜ Backward

### 6.1 Forward ì •ë¦¬

```
S = QK^T / âˆšd_k          # scores
A = softmax(S)           # attention weights
O = AV                   # output
```

### 6.2 Backward ìœ ë„

**âˆ‚L/âˆ‚V**:
```
O = AV
âˆ‚L/âˆ‚V = A^T Ã— âˆ‚L/âˆ‚O

Shape: [B, H, T, T]^T Ã— [B, H, T, d_v] = [B, H, T, d_v]
```

**âˆ‚L/âˆ‚A**:
```
O = AV
âˆ‚L/âˆ‚A = âˆ‚L/âˆ‚O Ã— V^T

Shape: [B, H, T, d_v] Ã— [B, H, d_v, T] = [B, H, T, T]
```

**âˆ‚L/âˆ‚S** (Softmax backward):
```
A = softmax(S)

âˆ‚L/âˆ‚S = A âŠ™ (âˆ‚L/âˆ‚A - Î£â±¼ âˆ‚L/âˆ‚Aâ±¼ Ã— Aâ±¼)

ë˜ëŠ” í–‰ë ¬ í˜•íƒœ:
âˆ‚L/âˆ‚S = A âŠ™ âˆ‚L/âˆ‚A - A âŠ™ (A Ã— âˆ‚L/âˆ‚A^T Ã— 1)
```

**âˆ‚L/âˆ‚Q, âˆ‚L/âˆ‚K**:
```
S = QK^T / âˆšd_k

âˆ‚L/âˆ‚Q = (âˆ‚L/âˆ‚S Ã— K) / âˆšd_k
âˆ‚L/âˆ‚K = (âˆ‚L/âˆ‚S^T Ã— Q) / âˆšd_k
```

### 6.3 ë©”ëª¨ë¦¬ íš¨ìœ¨ì  êµ¬í˜„ (FlashAttention)

```
ê¸°ì¡´: S, A ì „ì²´ ì €ì¥ â†’ O(TÂ²) ë©”ëª¨ë¦¬

FlashAttention:
- S, Aë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ê³„ì‚°
- ì „ì²´ ì €ì¥ ì—†ì´ online softmax
- Backwardì—ì„œ ì¬ê³„ì‚°

ë©”ëª¨ë¦¬: O(TÂ²) â†’ O(T)
```

---

## 7. Attention ë³€í˜•ë“¤

### 7.1 Additive Attention (Bahdanau)

```
score(q, k) = v^T Ã— tanh(W_q Ã— q + W_k Ã— k)

íŠ¹ì§•:
- ë¹„ì„ í˜• ìœ ì‚¬ë„
- Dot-productë³´ë‹¤ í‘œí˜„ë ¥ ë†’ìŒ
- ê³„ì‚° ë¹„ìš© ë†’ìŒ (ì‹¤ë¬´ì—ì„œ ì˜ ì•ˆ ì”€)
```

### 7.2 Relative Position Attention

```
score(q_i, k_j) = q_i Â· k_j + q_i Â· r_{i-j} + u Â· k_j + v Â· r_{i-j}

r_{i-j}: ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©
u, v: í•™ìŠµ íŒŒë¼ë¯¸í„°

ì¥ì : ê¸¸ì´ ì™¸ì‚½ (í•™ìŠµ ê¸¸ì´ë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬)
```

### 7.3 Rotary Position Embedding (RoPE)

```
q_m = R_m Ã— q    # m ìœ„ì¹˜ì˜ íšŒì „ ì ìš©
k_n = R_n Ã— k    # n ìœ„ì¹˜ì˜ íšŒì „ ì ìš©

q_m Â· k_n = q Â· R_m^T Ã— R_n Ã— k = q Â· R_{n-m} Ã— k

ì¥ì :
- ìƒëŒ€ ìœ„ì¹˜ ì •ë³´ê°€ ë‚´ì ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ê¸¸ì´ ì™¸ì‚½ì— ìœ ë¦¬
- LLaMA, Qwen ë“±ì—ì„œ ì‚¬ìš©
```

### 7.4 Grouped Query Attention (GQA)

```
ê¸°ì¡´ MHA: Hê°œ head, ê°ê° Q, K, V
GQA: Hê°œ Q head, Gê°œ K/V head (G < H)

ì˜ˆ: H=32, G=8
- ê° Q head ê·¸ë£¹ì´ K/Vë¥¼ ê³µìœ 
- ë©”ëª¨ë¦¬ ì ˆê° (KV cache ê°ì†Œ)
```

---

## 8. ê³„ì‚° ë³µì¡ë„ ë¶„ì„

### 8.1 ì‹œê°„ ë³µì¡ë„

```
Q, K, V: [B, T, d]
Attention ì—°ì‚°: O(BTÂ²d)

ë¶„ì„:
- QK^T: [B, T, d] Ã— [B, d, T] = O(BTÂ²d)
- Softmax: O(BTÂ²)
- (Softmax)V: [B, T, T] Ã— [B, T, d] = O(BTÂ²d)

Tê°€ ì§€ë°°ì  â†’ O(TÂ²)
```

### 8.2 ê³µê°„ ë³µì¡ë„

```
Attention matrix: [B, H, T, T]
ë©”ëª¨ë¦¬: O(BHTÂ²)

ì˜ˆ: B=32, H=32, T=4096
â†’ 32 Ã— 32 Ã— 4096Â² Ã— 4 bytes = 68 GB (!)
```

### 8.3 íš¨ìœ¨ì  Attention

| ë°©ë²• | ì‹œê°„ | ê³µê°„ | íŠ¹ì§• |
|:-----|:-----|:-----|:-----|
| ê¸°ë³¸ | O(TÂ²) | O(TÂ²) | ì •í™• |
| FlashAttention | O(TÂ²) | O(T) | IO ìµœì í™” |
| Sparse | O(TâˆšT) | O(T) | ê·¼ì‚¬ |
| Linear | O(T) | O(T) | ê·¼ì‚¬ |

---

## 9. ìˆ˜í•™ì  í•´ì„

### 9.1 Kernel ê´€ì 

```
Attention(Q, K, V) = softmax(QK^T) Ã— V

Kernel trick ì ìš©:
K(q, k) = exp(q Â· k / âˆšd)

softmax_i = K(q, k_i) / Î£â±¼ K(q, k_j)

â†’ RBF kernelì˜ ê·¼ì‚¬ë¡œ í•´ì„ ê°€ëŠ¥
```

### 9.2 ì •ë³´ ê²€ìƒ‰ ê´€ì 

```
Query: ê²€ìƒ‰ ì¿¼ë¦¬
Key: ë¬¸ì„œ ì¸ë±ìŠ¤
Value: ë¬¸ì„œ ë‚´ìš©

Attention = weighted retrieval
â†’ ì—°ì†ì ì¸ ì •ë³´ ê²€ìƒ‰
```

### 9.3 ê·¸ë˜í”„ ê´€ì 

```
Attention matrix AëŠ” ê°€ì¤‘ ì¸ì ‘ í–‰ë ¬

A_ij = ë…¸ë“œ i â†’ ë…¸ë“œ jì˜ ì—°ê²° ê°•ë„

Softmax â†’ ê° ë…¸ë“œì—ì„œ ë‚˜ê°€ëŠ” ì—£ì§€ì˜ í•© = 1
â†’ í™•ë¥ ì  ê·¸ë˜í”„ ìˆœíšŒ
```

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ ìˆ˜ì‹

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

êµ¬ì„±ìš”ì†Œ:
- QK^T: ìœ ì‚¬ë„ ê³„ì‚°
- âˆšd_k: ìŠ¤ì¼€ì¼ë§ (ë¶„ì‚° ì •ê·œí™”)
- softmax: í™•ë¥  ë¶„í¬ ë³€í™˜
- Ã—V: ê°€ì¤‘í•©
```

### 10.2 Multi-Head

```
MultiHead = Concat(head_1, ..., head_h) Ã— W_O
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

ì¥ì : ë‹¤ì–‘í•œ ê´€ê³„ ë™ì‹œ í•™ìŠµ
```

### 10.3 Causal Mask

```
í•˜ì‚¼ê° ë§ˆìŠ¤í¬ë¡œ ë¯¸ë˜ í† í° ì°¨ë‹¨
â†’ ìê¸°íšŒê·€ ìƒì„± ì§€ì›
```

### 10.4 ë³µì¡ë„

```
ì‹œê°„: O(TÂ²d)
ê³µê°„: O(TÂ²)

â†’ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë³‘ëª©
â†’ FlashAttention, Sparse Attention ë“±ìœ¼ë¡œ í•´ê²°
```

---

> ğŸ’¡ **ë³¸ë¬¸ ì—°ê²°**
> - [1.2 Transformer ì•„í‚¤í…ì²˜](../../01_ë”¥ëŸ¬ë‹_Transformer_ê¸°ì´ˆ/02_Transformer_ì•„í‚¤í…ì²˜.md)
> - [ë¶€ë¡ A1: ì„ í˜•ëŒ€ìˆ˜](../A_ìˆ˜í•™_ê¸°ì´ˆ/A1_ì„ í˜•ëŒ€ìˆ˜.md)
> - [ë¶€ë¡ B2: Backpropagation ìœ ë„](B2_Backpropagation_ìœ ë„.md)

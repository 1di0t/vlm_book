# B2. Backpropagation ìœ ë„

> Neural Network í•™ìŠµì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì¸ ì—­ì „íŒŒì˜ ìˆ˜í•™ì  ìœ ë„

---

## 1. ê°œìš”

### 1.1 Backpropagationì´ë€?

**ëª©ì **: ì†ì‹¤ í•¨ìˆ˜ Lì— ëŒ€í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°

```
âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚Wâ‚‚, ..., âˆ‚L/âˆ‚Wâ‚™ ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°
```

**í•µì‹¬ ì•„ì´ë””ì–´**: Chain Ruleì„ ì—­ë°©í–¥ìœ¼ë¡œ ì ìš©

### 1.2 ì™œ í•„ìš”í•œê°€?

**Naive ì ‘ê·¼ (ìˆ˜ì¹˜ ë¯¸ë¶„)**:
```
íŒŒë¼ë¯¸í„° ìˆ˜: Nê°œ
ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ 2ë²ˆì˜ forward pass í•„ìš”
ì´ ë³µì¡ë„: O(2N Ã— Forward)
```

**Backpropagation**:
```
Forward 1ë²ˆ + Backward 1ë²ˆ
ì´ ë³µì¡ë„: O(2 Ã— Forward)
```

GPT-3 (175B íŒŒë¼ë¯¸í„°) ê¸°ì¤€:
- ìˆ˜ì¹˜ ë¯¸ë¶„: 3500ì–µ ë²ˆì˜ forward pass
- Backprop: 2ë²ˆ

---

## 2. ë‹¨ì¼ ë‰´ëŸ°ì—ì„œì˜ ìœ ë„

### 2.1 Setup

```
ì…ë ¥: x âˆˆ â„
ê°€ì¤‘ì¹˜: w âˆˆ â„
í¸í–¥: b âˆˆ â„
ì •ë‹µ: y âˆˆ â„

Forward:
z = wx + b          (pre-activation)
a = Ïƒ(z)            (activation, Ïƒ = sigmoid)
L = (1/2)(a - y)Â²   (MSE loss)
```

### 2.2 Forward Pass ê³„ì‚°

```
x = 2, w = 0.5, b = 0.1, y = 1

z = 0.5 Ã— 2 + 0.1 = 1.1
a = Ïƒ(1.1) = 1/(1 + e^(-1.1)) â‰ˆ 0.75
L = (1/2)(0.75 - 1)Â² = 0.03125
```

### 2.3 Backward Pass ìœ ë„

**ëª©í‘œ**: âˆ‚L/âˆ‚w ê³„ì‚°

**Chain Rule ì ìš©**:
```
âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚w
```

**ê° í•­ ê³„ì‚°**:

1. **âˆ‚L/âˆ‚a** (ì†ì‹¤ â†’ ì¶œë ¥)
```
L = (1/2)(a - y)Â²
âˆ‚L/âˆ‚a = a - y = 0.75 - 1 = -0.25
```

2. **âˆ‚a/âˆ‚z** (ì¶œë ¥ â†’ pre-activation)
```
a = Ïƒ(z)
âˆ‚a/âˆ‚z = Ïƒ(z)(1 - Ïƒ(z)) = 0.75 Ã— 0.25 = 0.1875
```

3. **âˆ‚z/âˆ‚w** (pre-activation â†’ ê°€ì¤‘ì¹˜)
```
z = wx + b
âˆ‚z/âˆ‚w = x = 2
```

**ìµœì¢… ê²°ê³¼**:
```
âˆ‚L/âˆ‚w = (-0.25) Ã— 0.1875 Ã— 2 = -0.09375
```

**í¸í–¥ì— ëŒ€í•œ gradient**:
```
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚b
      = (-0.25) Ã— 0.1875 Ã— 1 = -0.046875
```

---

## 3. 2ì¸µ Neural Network ìœ ë„

### 3.1 Architecture

```
ì…ë ¥: x âˆˆ â„â¿ (nì°¨ì› ë²¡í„°)
ì€ë‹‰ì¸µ: h = m ë‰´ëŸ°
ì¶œë ¥ì¸µ: o = k í´ë˜ìŠ¤

ì¸µ1: Wâ‚ âˆˆ â„â¿Ë£áµ, bâ‚ âˆˆ â„áµ
ì¸µ2: Wâ‚‚ âˆˆ â„áµË£áµ, bâ‚‚ âˆˆ â„áµ
```

### 3.2 Forward Pass

```
# ì¸µ 1
zâ‚ = Wâ‚áµ€x + bâ‚        # [m]
aâ‚ = ReLU(zâ‚)          # [m]

# ì¸µ 2
zâ‚‚ = Wâ‚‚áµ€aâ‚ + bâ‚‚       # [k]
Å· = softmax(zâ‚‚)        # [k]

# ì†ì‹¤
L = CrossEntropy(y, Å·) = -Î£áµ¢ yáµ¢ log(Å·áµ¢)
```

### 3.3 Backward Pass ìœ ë„

#### Step 1: ì¶œë ¥ì¸µ gradient (âˆ‚L/âˆ‚zâ‚‚)

Softmax + Cross-Entropyì˜ ê²°í•© ë¯¸ë¶„:

```
âˆ‚L/âˆ‚zâ‚‚ = Å· - y    # [k] ë²¡í„°

ìœ ë„:
L = -Î£áµ¢ yáµ¢ log(Å·áµ¢)
Å·áµ¢ = softmax(zâ‚‚)áµ¢ = exp(zâ‚‚áµ¢) / Î£â±¼ exp(zâ‚‚â±¼)

âˆ‚L/âˆ‚zâ‚‚â±¼ = -Î£áµ¢ yáµ¢ Ã— (1/Å·áµ¢) Ã— âˆ‚Å·áµ¢/âˆ‚zâ‚‚â±¼

Softmax Jacobian:
âˆ‚Å·áµ¢/âˆ‚zâ‚‚â±¼ = Å·áµ¢(Î´áµ¢â±¼ - Å·â±¼)

ëŒ€ì… í›„ ì •ë¦¬:
âˆ‚L/âˆ‚zâ‚‚â±¼ = Å·â±¼ - yâ±¼
```

#### Step 2: Wâ‚‚ì˜ gradient (âˆ‚L/âˆ‚Wâ‚‚)

```
zâ‚‚ = Wâ‚‚áµ€aâ‚ + bâ‚‚

âˆ‚L/âˆ‚Wâ‚‚ = aâ‚ Ã— (âˆ‚L/âˆ‚zâ‚‚)áµ€    # [m, k] í–‰ë ¬

ë°°ì¹˜ ë²„ì „ (Bê°œ ìƒ˜í”Œ):
âˆ‚L/âˆ‚Wâ‚‚ = (1/B) Ã— Aâ‚áµ€ Ã— (Å¶ - Y)    # [m, k]
```

#### Step 3: bâ‚‚ì˜ gradient (âˆ‚L/âˆ‚bâ‚‚)

```
âˆ‚L/âˆ‚bâ‚‚ = âˆ‚L/âˆ‚zâ‚‚ = Å· - y    # [k]

ë°°ì¹˜ ë²„ì „:
âˆ‚L/âˆ‚bâ‚‚ = (1/B) Ã— Î£áµ¦ (Å·áµ¦ - yáµ¦)    # [k]
```

#### Step 4: ì€ë‹‰ì¸µìœ¼ë¡œ ì „íŒŒ (âˆ‚L/âˆ‚aâ‚)

```
âˆ‚L/âˆ‚aâ‚ = Wâ‚‚ Ã— âˆ‚L/âˆ‚zâ‚‚    # [m]

zâ‚‚ = Wâ‚‚áµ€aâ‚ ì´ë¯€ë¡œ
âˆ‚zâ‚‚/âˆ‚aâ‚ = Wâ‚‚áµ€
ë”°ë¼ì„œ âˆ‚L/âˆ‚aâ‚ = (Wâ‚‚áµ€)áµ€ Ã— âˆ‚L/âˆ‚zâ‚‚ = Wâ‚‚ Ã— âˆ‚L/âˆ‚zâ‚‚
```

#### Step 5: ReLU í†µê³¼ (âˆ‚L/âˆ‚zâ‚)

```
âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ âŠ™ ReLU'(zâ‚)    # [m]

ReLU'(z) = {
    1 if z > 0
    0 if z â‰¤ 0
}

ì¦‰, âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ âŠ™ (zâ‚ > 0)
```

#### Step 6: Wâ‚ì˜ gradient (âˆ‚L/âˆ‚Wâ‚)

```
âˆ‚L/âˆ‚Wâ‚ = x Ã— (âˆ‚L/âˆ‚zâ‚)áµ€    # [n, m]

ë°°ì¹˜ ë²„ì „:
âˆ‚L/âˆ‚Wâ‚ = (1/B) Ã— Xáµ€ Ã— (âˆ‚L/âˆ‚Zâ‚)    # [n, m]
```

### 3.4 ìˆ˜ì‹ ìš”ì•½

| ë³€ìˆ˜ | Gradient | Shape |
|:-----|:---------|:------|
| zâ‚‚ | Å· - y | [B, k] |
| Wâ‚‚ | (1/B) Ã— Aâ‚áµ€ Ã— (Å¶ - Y) | [m, k] |
| bâ‚‚ | (1/B) Ã— Î£áµ¦ (Å·áµ¦ - yáµ¦) | [k] |
| aâ‚ | (Å¶ - Y) Ã— Wâ‚‚áµ€ | [B, m] |
| zâ‚ | âˆ‚L/âˆ‚aâ‚ âŠ™ (Zâ‚ > 0) | [B, m] |
| Wâ‚ | (1/B) Ã— Xáµ€ Ã— âˆ‚L/âˆ‚Zâ‚ | [n, m] |
| bâ‚ | (1/B) Ã— Î£áµ¦ âˆ‚L/âˆ‚zâ‚áµ¦ | [m] |

---

## 4. ì¼ë°˜í™”: Lì¸µ Network

### 4.1 Forward Pass (ì¼ë°˜ í˜•íƒœ)

```
for l = 1, 2, ..., L:
    zâ½Ë¡â¾ = Wâ½Ë¡â¾áµ€aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
    aâ½Ë¡â¾ = Ïƒâ½Ë¡â¾(zâ½Ë¡â¾)

ì—¬ê¸°ì„œ aâ½â°â¾ = x (ì…ë ¥)
```

### 4.2 Backward Pass (ì¼ë°˜ í˜•íƒœ)

```
# ì¶œë ¥ì¸µ
Î´â½á´¸â¾ = âˆ‚L/âˆ‚zâ½á´¸â¾ = (âˆ‚L/âˆ‚aâ½á´¸â¾) âŠ™ Ïƒ'â½á´¸â¾(zâ½á´¸â¾)

# ì—­ì „íŒŒ (l = L-1, L-2, ..., 1)
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾Î´â½Ë¡âºÂ¹â¾) âŠ™ Ïƒ'â½Ë¡â¾(zâ½Ë¡â¾)

# Gradient ê³„ì‚°
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = aâ½Ë¡â»Â¹â¾(Î´â½Ë¡â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾
```

### 4.3 ì˜ì‚¬ì½”ë“œ

```python
def backprop(network, x, y):
    """
    Backpropagation ì•Œê³ ë¦¬ì¦˜

    Args:
        network: ì¸µë“¤ì˜ ë¦¬ìŠ¤íŠ¸ [(W1,b1), (W2,b2), ...]
        x: ì…ë ¥ [batch, input_dim]
        y: ì •ë‹µ [batch, output_dim]

    Returns:
        gradients: ê° ì¸µì˜ gradient
    """
    L = len(network)
    gradients = []

    # Forward pass (í™œì„±í™” ê°’ ì €ì¥)
    activations = [x]  # aâ½â°â¾ = x
    pre_activations = []

    a = x
    for l, (W, b) in enumerate(network):
        z = a @ W + b
        pre_activations.append(z)

        if l == L - 1:  # ë§ˆì§€ë§‰ ì¸µ
            a = softmax(z)
        else:
            a = relu(z)
        activations.append(a)

    # Backward pass
    # ì¶œë ¥ì¸µ: softmax + cross-entropy
    delta = activations[-1] - y  # Å· - y

    for l in reversed(range(L)):
        W, b = network[l]
        a_prev = activations[l]

        # Gradient ê³„ì‚°
        dW = a_prev.T @ delta / len(x)
        db = delta.mean(axis=0)
        gradients.insert(0, (dW, db))

        # ì´ì „ ì¸µìœ¼ë¡œ ì „íŒŒ
        if l > 0:
            delta = (delta @ W.T) * relu_derivative(pre_activations[l-1])

    return gradients
```

---

## 5. Computational Graph ê´€ì 

### 5.1 ê°œë…

Neural Networkë¥¼ ì—°ì‚°ì˜ ê·¸ë˜í”„ë¡œ í‘œí˜„:

```
     x â”€â”€â†’ [Ã—Wâ‚] â”€â”€â†’ zâ‚ â”€â”€â†’ [ReLU] â”€â”€â†’ aâ‚
                                        â”‚
                                        â†“
                              [Ã—Wâ‚‚] â”€â”€â†’ zâ‚‚
                                        â”‚
            y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [Loss] â”€â”€â†’ L
```

### 5.2 Forward Mode vs Backward Mode

**Forward Mode (ì „ë°© ë¯¸ë¶„)**:
- ì…ë ¥ â†’ ì¶œë ¥ ë°©í–¥ìœ¼ë¡œ ë¯¸ë¶„ ì „íŒŒ
- í•˜ë‚˜ì˜ ì…ë ¥ ë³€ìˆ˜ì— ëŒ€í•œ ëª¨ë“  ì¶œë ¥ì˜ ë¯¸ë¶„
- ì…ë ¥ì´ ì ê³  ì¶œë ¥ì´ ë§ì„ ë•Œ ìœ ë¦¬

**Backward Mode (ì—­ë°©í–¥ ë¯¸ë¶„)**:
- ì¶œë ¥ â†’ ì…ë ¥ ë°©í–¥ìœ¼ë¡œ ë¯¸ë¶„ ì „íŒŒ
- í•˜ë‚˜ì˜ ì¶œë ¥ì— ëŒ€í•œ ëª¨ë“  ì…ë ¥ì˜ ë¯¸ë¶„
- **Neural Networkì— ì í•©** (ì¶œë ¥=ì†ì‹¤ 1ê°œ, ì…ë ¥=íŒŒë¼ë¯¸í„° ìˆ˜ì–µ ê°œ)

### 5.3 Local Gradient

ê° ì—°ì‚° ë…¸ë“œëŠ” local gradientë§Œ ê³„ì‚°:

| ì—°ì‚° | Forward | Local Gradient |
|:-----|:--------|:---------------|
| ë§ì…ˆ | c = a + b | âˆ‚c/âˆ‚a = 1, âˆ‚c/âˆ‚b = 1 |
| ê³±ì…ˆ | c = a Ã— b | âˆ‚c/âˆ‚a = b, âˆ‚c/âˆ‚b = a |
| ReLU | c = max(0, a) | âˆ‚c/âˆ‚a = 1 if a > 0 else 0 |
| Sigmoid | c = Ïƒ(a) | âˆ‚c/âˆ‚a = c(1-c) |
| MatMul | C = AB | âˆ‚L/âˆ‚A = (âˆ‚L/âˆ‚C)Báµ€ |

---

## 6. í–‰ë ¬ ë¯¸ë¶„ì˜ ìƒì„¸

### 6.1 ì„ í˜• ë³€í™˜ z = Wx + b

**Forward**:
```
z: [batch, out]
W: [in, out]
x: [batch, in]
b: [out]

z = x @ W + b
```

**Backward**:
```
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z @ Wáµ€      # [batch, in]
âˆ‚L/âˆ‚W = xáµ€ @ âˆ‚L/âˆ‚z      # [in, out]
âˆ‚L/âˆ‚b = sum(âˆ‚L/âˆ‚z, axis=0)  # [out]
```

### 6.2 ìœ ë„ (âˆ‚L/âˆ‚W)

```
z_ij = Î£â‚– x_ik Ã— W_kj + b_j

âˆ‚z_ij/âˆ‚W_mn = x_im Ã— Î´_jn

âˆ‚L/âˆ‚W_mn = Î£áµ¢â±¼ (âˆ‚L/âˆ‚z_ij) Ã— (âˆ‚z_ij/âˆ‚W_mn)
         = Î£áµ¢ (âˆ‚L/âˆ‚z_in) Ã— x_im
         = Î£áµ¢ x_im Ã— (âˆ‚L/âˆ‚z_in)
         = (xáµ€ @ âˆ‚L/âˆ‚z)_mn
```

### 6.3 PyTorchì—ì„œì˜ ìë™ ë¯¸ë¶„

```python
import torch

# ìë™ ë¯¸ë¶„ í™œì„±í™”
x = torch.randn(32, 10, requires_grad=False)
W = torch.randn(10, 5, requires_grad=True)
b = torch.randn(5, requires_grad=True)

# Forward
z = x @ W + b
a = torch.relu(z)
loss = a.sum()

# Backward (ìë™ ê³„ì‚°)
loss.backward()

# Gradient í™•ì¸
print(W.grad.shape)  # [10, 5]
print(b.grad.shape)  # [5]
```

---

## 7. Gradient Flow ë¬¸ì œ

### 7.1 Vanishing Gradient

**ì›ì¸**: í™œì„±í™” í•¨ìˆ˜ ë¯¸ë¶„ê°’ì´ 1ë³´ë‹¤ ì‘ìŒ

```
Sigmoid: Ïƒ'(x) â‰¤ 0.25
Tanh: tanh'(x) â‰¤ 1

Lì¸µì„ í†µê³¼í•˜ë©´: gradient â‰ˆ 0.25^L
L=10ì´ë©´: gradient â‰ˆ 10^(-6)
```

**í•´ê²°ì±…**:
- ReLU ì‚¬ìš© (ë¯¸ë¶„ = 1)
- Residual Connection
- Batch/Layer Normalization
- ì ì ˆí•œ ì´ˆê¸°í™”

### 7.2 Exploding Gradient

**ì›ì¸**: gradientê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€

```
Wì˜ ìµœëŒ€ ê³ ìœ ê°’ > 1ì´ë©´ gradient í­ë°œ
```

**í•´ê²°ì±…**:
- Gradient Clipping
- ì ì ˆí•œ ì´ˆê¸°í™”
- Layer Normalization

### 7.3 Gradient Clipping êµ¬í˜„

```python
def clip_gradient(grads, max_norm):
    """
    Gradientì˜ normì„ ì œí•œ

    Args:
        grads: gradient ë¦¬ìŠ¤íŠ¸
        max_norm: ìµœëŒ€ norm
    """
    total_norm = 0
    for g in grads:
        total_norm += (g ** 2).sum()
    total_norm = total_norm ** 0.5

    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1:
        for g in grads:
            g *= clip_coef

    return grads
```

---

## 8. ì™„ì „í•œ Python êµ¬í˜„

### 8.1 ì¸µ í´ë˜ìŠ¤

```python
import numpy as np

class Linear:
    """ì„ í˜• ì¸µ"""
    def __init__(self, in_features, out_features):
        # Xavier ì´ˆê¸°í™”
        self.W = np.random.randn(in_features, out_features) * np.sqrt(2 / in_features)
        self.b = np.zeros(out_features)
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        return x @ self.W + self.b

    def backward(self, dout):
        self.dW = self.x.T @ dout
        self.db = dout.sum(axis=0)
        return dout @ self.W.T


class ReLU:
    """ReLU í™œì„±í™”"""
    def forward(self, x):
        self.mask = (x > 0)
        return x * self.mask

    def backward(self, dout):
        return dout * self.mask


class Softmax:
    """Softmax í™œì„±í™”"""
    def forward(self, x):
        exp_x = np.exp(x - x.max(axis=1, keepdims=True))
        self.out = exp_x / exp_x.sum(axis=1, keepdims=True)
        return self.out


class CrossEntropyLoss:
    """Cross-Entropy ì†ì‹¤"""
    def forward(self, pred, target):
        self.pred = pred
        self.target = target
        batch_size = pred.shape[0]
        # targetì´ one-hotì¼ ë•Œ
        loss = -np.sum(target * np.log(pred + 1e-8)) / batch_size
        return loss

    def backward(self):
        batch_size = self.pred.shape[0]
        return (self.pred - self.target) / batch_size
```

### 8.2 Network í´ë˜ìŠ¤

```python
class NeuralNetwork:
    """2ì¸µ Neural Network"""
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.layers = [
            Linear(input_dim, hidden_dim),
            ReLU(),
            Linear(hidden_dim, output_dim),
            Softmax()
        ]
        self.loss_fn = CrossEntropyLoss()

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self):
        dout = self.loss_fn.backward()
        for layer in reversed(self.layers[:-1]):  # softmax ì œì™¸
            dout = layer.backward(dout)

    def update(self, lr):
        for layer in self.layers:
            if hasattr(layer, 'W'):
                layer.W -= lr * layer.dW
                layer.b -= lr * layer.db

    def train_step(self, x, y, lr=0.01):
        # Forward
        pred = self.forward(x)
        loss = self.loss_fn.forward(pred, y)

        # Backward
        self.backward()

        # Update
        self.update(lr)

        return loss
```

### 8.3 í•™ìŠµ ë£¨í”„

```python
# ë°ì´í„° ìƒì„± (XOR ë¬¸ì œ)
X = np.array([[0,0], [0,1], [1,0], [1,1]])
Y = np.array([[1,0], [0,1], [0,1], [1,0]])  # one-hot

# ëª¨ë¸ ìƒì„±
model = NeuralNetwork(input_dim=2, hidden_dim=4, output_dim=2)

# í•™ìŠµ
for epoch in range(1000):
    loss = model.train_step(X, Y, lr=0.5)
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# ì˜ˆì¸¡
pred = model.forward(X)
print("Predictions:", pred.argmax(axis=1))
print("Labels:", Y.argmax(axis=1))
```

---

## 9. Transformerì—ì„œì˜ Backpropagation

### 9.1 Attentionì˜ Backward

```
Forward:
scores = Q @ K.T / sqrt(d_k)
weights = softmax(scores)
output = weights @ V

Backward:
âˆ‚L/âˆ‚V = weights.T @ âˆ‚L/âˆ‚output
âˆ‚L/âˆ‚weights = âˆ‚L/âˆ‚output @ V.T
âˆ‚L/âˆ‚scores = softmax_backward(âˆ‚L/âˆ‚weights)
âˆ‚L/âˆ‚Q = (âˆ‚L/âˆ‚scores @ K) / sqrt(d_k)
âˆ‚L/âˆ‚K = (âˆ‚L/âˆ‚scores.T @ Q) / sqrt(d_k)
```

### 9.2 ë©”ëª¨ë¦¬ íš¨ìœ¨ì  Attention Backward

```
# FlashAttentionì˜ í•µì‹¬: ì¤‘ê°„ attention matrix ì €ì¥ ì•ˆ í•¨

Forward:
- Q, K, Vë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
- softmaxë¥¼ onlineìœ¼ë¡œ ê³„ì‚°
- ì €ì¥: Q, K, V, output, softmax ì •ê·œí™” ìƒìˆ˜ë§Œ

Backward:
- ì €ì¥ëœ ê°’ìœ¼ë¡œ attention weights ì¬ê³„ì‚°
- ë©”ëª¨ë¦¬: O(N) vs ê¸°ì¡´ O(NÂ²)
```

---

## 10. ìš”ì•½

### 10.1 Backpropagation í•µì‹¬ ê³µì‹

| ì¸µ | Forward | Backward (âˆ‚L/âˆ‚input) |
|:---|:--------|:---------------------|
| Linear | y = Wx + b | dW = xáµ€Â·dy, db = Î£dy, dx = dyÂ·Wáµ€ |
| ReLU | y = max(0, x) | dx = dy âŠ™ (x > 0) |
| Softmax | y = softmax(x) | (Combined with CE) |
| CE Loss | L = -Î£yÂ·log(Å·) | dz = Å· - y |

### 10.2 êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Forwardì—ì„œ ì¤‘ê°„ê°’ ì €ì¥ (backwardì—ì„œ í•„ìš”)
- [ ] BackwardëŠ” ì—­ìˆœìœ¼ë¡œ ì§„í–‰
- [ ] Shape ì¼ì¹˜ í™•ì¸
- [ ] Gradient Checkingìœ¼ë¡œ ê²€ì¦
- [ ] Gradient Clipping ì ìš©

### 10.3 PyTorchì—ì„œì˜ í™œìš©

```python
# ìˆ˜ë™ êµ¬í˜„ ëŒ€ì‹  autograd ì‚¬ìš©
model = nn.Sequential(
    nn.Linear(input_dim, hidden_dim),
    nn.ReLU(),
    nn.Linear(hidden_dim, output_dim)
)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ ë£¨í”„
for x, y in dataloader:
    optimizer.zero_grad()           # gradient ì´ˆê¸°í™”
    output = model(x)               # forward
    loss = criterion(output, y)     # ì†ì‹¤ ê³„ì‚°
    loss.backward()                 # backward (ìë™!)
    optimizer.step()                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
```

---

> ğŸ’¡ **ë³¸ë¬¸ ì—°ê²°**
> - [1.1 ìˆ˜í•™ì  ê¸°ì´ˆ](../../01_ë”¥ëŸ¬ë‹_Transformer_ê¸°ì´ˆ/01_ìˆ˜í•™ì _ê¸°ì´ˆ.md)
> - [ë¶€ë¡ A2: ë¯¸ì ë¶„](../A_ìˆ˜í•™_ê¸°ì´ˆ/A2_ë¯¸ì ë¶„.md)
> - [ë¶€ë¡ B1: Attention ìˆ˜ì‹ ìœ ë„](B1_Attention_ìˆ˜ì‹_ìœ ë„.md)

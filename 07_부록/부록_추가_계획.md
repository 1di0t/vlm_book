# 부록 추가 계획

## 목적

본문의 흐름을 방해하지 않으면서 수학적 기초, 증명, 심화 내용을 별도로 정리하여 필요 시 참조할 수 있도록 함.

---

## 폴더 구조

```
vlm_book/
├── 07_부록/
│   ├── 참고자료_정리.md          (기존)
│   ├── 부록_추가_계획.md         (현재 문서)
│   │
│   ├── A_수학_기초/              ← 신규 폴더
│   │   ├── A0_수학_기호_사전.md   ← 기호 정리
│   │   ├── A1_선형대수.md
│   │   ├── A2_미적분.md
│   │   ├── A3_확률통계.md
│   │   └── A4_최적화이론.md
│   │
│   ├── B_증명_유도/              ← 신규 폴더
│   │   ├── B1_Attention_수식_유도.md
│   │   ├── B2_Backpropagation_유도.md
│   │   ├── B3_Softmax_Gradient.md
│   │   └── B4_Cross_Entropy_유도.md
│   │
│   ├── C_구현_상세/              ← 신규 폴더
│   │   ├── C1_Transformer_from_scratch.md
│   │   ├── C2_LoRA_구현_상세.md
│   │   └── C3_KV_Cache_최적화.md
│   │
│   ├── D_실습_환경/              ← 신규 폴더
│   │   ├── D1_GPU_환경_설정.md
│   │   ├── D2_Docker_설정.md
│   │   └── D3_디버깅_가이드.md
│   │
│   ├── E_Vision_심화/            ← 신규 폴더 (Phase 2)
│   │   ├── E1_CNN_vs_Transformer.md
│   │   ├── E2_Vision_표현학습.md
│   │   └── E3_해상도_처리.md
│   │
│   ├── F_데이터_처리/            ← 신규 폴더 (Phase 3)
│   │   ├── F1_이미지_전처리.md
│   │   ├── F2_텍스트_처리.md
│   │   └── F3_데이터_포맷.md
│   │
│   ├── G_학습_최적화/            ← 신규 폴더 (Phase 4)
│   │   ├── G1_Optimizer_상세.md
│   │   ├── G2_LR_Schedule.md
│   │   ├── G3_Gradient_관리.md
│   │   └── G4_분산_학습.md
│   │
│   ├── H_양자화/                 ← 신규 폴더 (Phase 5)
│   │   ├── H1_양자화_이론.md
│   │   ├── H2_LLM_양자화.md
│   │   └── H3_KV_Cache_양자화.md
│   │
│   ├── I_서빙_심화/              ← 신규 폴더 (Phase 6)
│   │   ├── I1_추론_최적화.md
│   │   ├── I2_메모리_관리.md
│   │   ├── I3_배포_패턴.md
│   │   └── I4_모니터링.md
│   │
│   └── J_의료_OCR/               ← 신규 폴더 (SI 특화)
│       ├── J1_문서_유형별.md
│       ├── J2_전처리_파이프라인.md
│       ├── J3_한국어_특화.md
│       └── J4_품질_검증.md
```

---

## 부록 A: 수학 기초

### A0. 수학 기호 사전

**그리스 문자**

| 기호 | 이름 | 발음 | 용도 |
|------|------|------|------|
| α | alpha | 알파 | learning rate, attention weight |
| β | beta | 베타 | momentum, LoRA scaling |
| γ | gamma | 감마 | Layer Norm scale |
| δ | delta | 델타 | 오차, 변화량 |
| ε | epsilon | 엡실론 | 작은 상수 (수치 안정성) |
| η | eta | 에타 | learning rate |
| θ | theta | 세타 | 모델 파라미터 |
| λ | lambda | 람다 | 정규화 계수, 고유값 |
| μ | mu | 뮤 | 평균 |
| σ | sigma | 시그마 | 표준편차, 활성화 함수 |
| τ | tau | 타우 | temperature |
| φ | phi | 파이 | 활성화 함수, 특징 변환 |
| ω | omega | 오메가 | 가중치, 주파수 |

**미적분 기호**

| 기호 | 의미 | 예시 |
|------|------|------|
| ∂ | 편미분 | ∂L/∂W (L을 W로 편미분) |
| ∇ | gradient (nabla) | ∇f = [∂f/∂x₁, ∂f/∂x₂, ...] |
| d/dx | 전미분 | df/dx |
| ∫ | 적분 | ∫f(x)dx |
| Σ | 합 (summation) | Σᵢxᵢ = x₁ + x₂ + ... |
| ∏ | 곱 (product) | ∏ᵢxᵢ = x₁ × x₂ × ... |
| lim | 극한 | lim(x→0) f(x) |

**선형대수 기호**

| 기호 | 의미 | 예시 |
|------|------|------|
| × | 행렬 곱셈 | A × B |
| ⊙ | element-wise 곱 (Hadamard) | a ⊙ b |
| ⊗ | 텐서 곱 (Kronecker) | A ⊗ B |
| · | 내적 (dot product) | a · b = Σᵢaᵢbᵢ |
| ᵀ | 전치 (transpose) | Aᵀ |
| ⁻¹ | 역행렬 | A⁻¹ |
| ‖·‖ | 노름 (norm) | ‖x‖₂ = √(Σᵢxᵢ²) |
| det | 행렬식 | det(A) |
| tr | 대각합 (trace) | tr(A) = Σᵢaᵢᵢ |
| diag | 대각 행렬 | diag(a) |

**확률/통계 기호**

| 기호 | 의미 | 예시 |
|------|------|------|
| P(·) | 확률 | P(X=x) |
| E[·] | 기댓값 | E[X] = Σᵢxᵢp(xᵢ) |
| Var(·) | 분산 | Var(X) = E[(X-μ)²] |
| ~ | 분포를 따름 | X ~ N(0, 1) |
| ∝ | 비례 | P(x) ∝ exp(-x²) |
| ⊥ | 독립 | X ⊥ Y |
| | | 조건부 | P(A|B) |

**집합/논리 기호**

| 기호 | 의미 | 예시 |
|------|------|------|
| ∈ | 원소 | x ∈ ℝ |
| ⊂ | 부분집합 | A ⊂ B |
| ∪ | 합집합 | A ∪ B |
| ∩ | 교집합 | A ∩ B |
| ∀ | 모든 | ∀x ∈ ℝ |
| ∃ | 존재 | ∃x such that |
| ℝ | 실수 | x ∈ ℝⁿ |
| ℤ | 정수 | n ∈ ℤ |

**딥러닝 특화 기호**

| 기호 | 의미 | 용도 |
|------|------|------|
| W, w | weight | 가중치 행렬/벡터 |
| b | bias | 편향 |
| x | input | 입력 |
| y, ŷ | output, prediction | 출력, 예측 |
| L, ℒ | loss | 손실 함수 |
| h | hidden state | 은닉 상태 |
| Q, K, V | Query, Key, Value | Attention |
| d_model, d_k | dimension | 차원 |
| N | batch size 또는 sequence length | 컨텍스트에 따라 |
| T | sequence length (time) | 시퀀스 길이 |

**본문 수식 예시 해석**

```
∂L/∂w = (∂L/∂y) × (∂y/∂w)

해석:
- ∂L/∂w: 손실 L을 가중치 w로 편미분 (w가 L에 미치는 영향)
- ∂L/∂y: 손실 L을 출력 y로 편미분
- ∂y/∂w: 출력 y를 가중치 w로 편미분
- Chain Rule: 이 둘을 곱하면 전체 gradient
```

```
Attention(Q, K, V) = softmax(QK^T / √d_k) × V

해석:
- Q: Query 행렬 (내가 찾는 것)
- K: Key 행렬 (정보의 인덱스)
- V: Value 행렬 (실제 정보)
- K^T: K의 전치 행렬
- QK^T: Q와 K의 유사도 점수
- √d_k: 스케일링 (d_k는 key의 차원)
- softmax: 확률 분포로 변환
- × V: 확률로 V를 가중합
```

---

### A1. 선형대수

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| 행렬 곱셈 | 정의, 성질, 계산 복잡도 | Attention 연산 |
| 전치/역행렬 | 정의, 성질 | Q, K, V 변환 |
| 고유값/고유벡터 | 정의, 계산법, 기하학적 의미 | PCA, Attention 해석 |
| SVD | 특이값 분해 | LoRA low-rank 이해 |
| Norm | L1, L2, Frobenius | 정규화, 손실함수 |

### A2. 미적분

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| 편미분 | 정의, 표기법 | Gradient 계산 |
| Chain Rule | 단변수/다변수 | Backpropagation |
| Jacobian | 정의, 계산 | 층간 gradient 전파 |
| Hessian | 2차 미분 행렬 | 최적화 이론 |
| Taylor 전개 | 1차, 2차 근사 | 최적화 알고리즘 |

**Chain Rule 상세 예시:**
```
Neural Network: x → z₁ → a₁ → z₂ → a₂ → L

∂L/∂W₁ = ∂L/∂a₂ × ∂a₂/∂z₂ × ∂z₂/∂a₁ × ∂a₁/∂z₁ × ∂z₁/∂W₁

단계별:
1. ∂L/∂a₂ = d_loss(a₂, y)           # 손실함수 미분
2. ∂a₂/∂z₂ = d_activation(z₂)       # 활성화 함수 미분
3. ∂z₂/∂a₁ = W₂                      # 선형 변환
4. ∂a₁/∂z₁ = d_activation(z₁)       # 활성화 함수 미분
5. ∂z₁/∂W₁ = x                       # 입력
```

### A3. 확률/통계

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| 확률 분포 | Gaussian, Categorical, Bernoulli | 샘플링, 초기화 |
| 기댓값/분산 | 정의, 성질 | Batch Norm, Layer Norm |
| MLE | 최대우도추정 | Cross-Entropy 유도 |
| KL Divergence | 정의, 성질, 비대칭성 | Knowledge Distillation |
| 정보 이론 | 엔트로피, 상호정보량 | 손실함수 이해 |

### A4. 최적화 이론

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Gradient Descent | 수렴 조건, learning rate | 기본 학습 |
| Momentum | 지수이동평균 | Adam, AdamW |
| Adaptive LR | AdaGrad, RMSprop, Adam | 최적화 선택 |
| Weight Decay | L2 정규화와 관계 | AdamW |
| Learning Rate Schedule | Warmup, Cosine Decay | 학습 안정성 |

---

## 부록 B: 증명 및 유도

### B1. Attention 수식 유도

```
목표: Attention(Q, K, V) = softmax(QK^T / √d_k) × V

1. 유사도 측정
   - 내적 기반: score(q, k) = q · k
   - 문제: d_k가 커지면 내적 값이 커짐

2. Scaling 필요성
   - q, k ~ N(0, 1)일 때, q · k ~ N(0, d_k)
   - 분산이 d_k에 비례 → softmax gradient vanishing
   - 해결: √d_k로 나누어 분산을 1로 정규화

3. Softmax 적용
   - 확률 분포로 변환
   - 중요한 위치에 집중

4. Value 가중합
   - attention_weights × V
```

### B2. Backpropagation 유도

```
목표: ∂L/∂W 계산

Layer: z = Wx + b, a = σ(z)

Forward:
x → [W, b] → z → [σ] → a → ... → L

Backward (Chain Rule):
∂L/∂W = ∂L/∂a × ∂a/∂z × ∂z/∂W

구체적 계산:
1. ∂z/∂W = x^T (Jacobian)
2. ∂a/∂z = σ'(z) (element-wise)
3. ∂L/∂a = 다음 층에서 전파

최종: ∂L/∂W = (∂L/∂a ⊙ σ'(z)) × x^T
```

### B3. Softmax Gradient

```
softmax(x)_i = exp(x_i) / Σ_j exp(x_j)

Jacobian 계산:
∂softmax_i/∂x_j = {
  softmax_i(1 - softmax_i)  if i = j
  -softmax_i × softmax_j     if i ≠ j
}

행렬 형태:
J = diag(s) - s × s^T

여기서 s = softmax(x)
```

### B4. Cross-Entropy 유도

```
목표: H(p, q) = -Σ p(x) log q(x)

1. 정보량 정의
   I(x) = -log p(x)
   - 확률 낮은 사건 → 높은 정보량

2. 엔트로피
   H(p) = E_p[I(x)] = -Σ p(x) log p(x)
   - 분포의 불확실성

3. Cross-Entropy
   H(p, q) = -Σ p(x) log q(x)
   - q로 p를 인코딩할 때 필요한 비트 수

4. 분류 문제 적용
   - p = one-hot (정답)
   - q = softmax(logits) (예측)
   - H(p, q) = -log q(y_true)
```

---

## 부록 C: 구현 상세

### C1. Transformer from scratch

- 전체 Transformer 구현 (400줄)
- 단계별 shape 추적
- 메모리 사용량 계산

### C2. LoRA 구현 상세

- rank 선택 실험
- target modules 비교
- 병합(merge) 방법

### C3. KV Cache 최적화

- 메모리 계산
- PagedAttention 상세
- Sliding window 구현

---

## 부록 D: 실습 환경

### D1. GPU 환경 설정

- CUDA 버전 호환성
- PyTorch 설치
- vLLM 환경 구성

### D2. Docker 설정

- GPU 지원 Docker
- 모델 볼륨 마운트
- 멀티 GPU 설정

### D3. 디버깅 가이드

- OOM 해결
- NaN/Inf 디버깅
- Gradient 추적

---

## 부록 E: Vision 심화 (Phase 2 연결)

### E1. CNN vs Transformer 비교

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Inductive Bias | locality, translation equivariance | ViT 이해 |
| Receptive Field | CNN의 제한 vs Transformer의 global | Attention 장점 |
| 계산 복잡도 | O(n²) vs O(n) | 효율성 트레이드오프 |

### E2. Vision 표현 학습

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Contrastive Learning | SimCLR, MoCo, CLIP | CLIP 이해 |
| MAE | Masked Autoencoder | Self-supervised |
| DINO | Self-distillation | ViT 특성 |

### E3. 해상도 처리 기법

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Position Interpolation | 해상도 변경 시 | ViT 가변 해상도 |
| Sliding Window | Swin Transformer | 효율적 attention |
| Dynamic Resolution | Qwen2-VL 방식 | 문서 OCR |

---

## 부록 F: 데이터 처리 심화 (Phase 3 연결)

### F1. 이미지 전처리

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| 정규화 | ImageNet mean/std | 입력 전처리 |
| 증강 기법 | RandAugment, CutOut, MixUp | 데이터 증강 |
| 해상도 조정 | resize, padding, crop | 배치 처리 |

### F2. 텍스트 처리

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| BPE 알고리즘 상세 | 병합 과정, vocab 구축 | Tokenization |
| 특수 토큰 | BOS, EOS, PAD, UNK | 시퀀스 처리 |
| Chat Template | 역할별 포맷팅 | Instruction tuning |

### F3. 데이터 포맷

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Conversation Format | OpenAI, ShareGPT 형식 | VLM 학습 데이터 |
| JSONL 처리 | 대용량 데이터 스트리밍 | 효율적 로딩 |
| WebDataset | tar 기반 포맷 | 분산 학습 |

---

## 부록 G: 학습 최적화 심화 (Phase 4 연결)

### G1. Optimizer 상세

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Adam 수식 | 모멘텀, 적응적 학습률 | 기본 optimizer |
| AdamW | weight decay 분리 | 권장 optimizer |
| 8-bit Adam | bitsandbytes | QLoRA |

```
Adam 업데이트:
m_t = β₁ × m_{t-1} + (1 - β₁) × g_t      # 1차 모멘텀
v_t = β₂ × v_{t-1} + (1 - β₂) × g_t²     # 2차 모멘텀
m̂_t = m_t / (1 - β₁ᵗ)                    # bias 보정
v̂_t = v_t / (1 - β₂ᵗ)                    # bias 보정
θ_t = θ_{t-1} - η × m̂_t / (√v̂_t + ε)   # 파라미터 업데이트
```

### G2. Learning Rate Schedule

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Linear Warmup | 초기 불안정 방지 | 학습 안정성 |
| Cosine Annealing | 점진적 감소 | 수렴 품질 |
| One-Cycle | 삼각형 스케줄 | 빠른 학습 |

### G3. Gradient 관리

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Gradient Clipping | max_grad_norm | 안정성 |
| Gradient Accumulation | 가상 배치 크기 | 메모리 절약 |
| Gradient Checkpointing | 메모리-연산 트레이드오프 | 대규모 모델 |

### G4. 분산 학습

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| DDP | 데이터 병렬 | 기본 분산 |
| FSDP | 완전 샤딩 | 메모리 효율 |
| DeepSpeed ZeRO | Stage 1/2/3 | 대규모 학습 |
| Tensor Parallel | 모델 분할 | 초대형 모델 |

---

## 부록 H: 양자화 심화 (Phase 5 연결)

### H1. 양자화 이론

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| 양자화 기초 | FP32 → INT8/4 변환 | 기본 개념 |
| Symmetric vs Asymmetric | 범위 매핑 | 정밀도 |
| Per-tensor vs Per-channel | 세분화 수준 | 정확도 |

### H2. LLM 양자화 기법

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| GPTQ | Post-training, 블록 단위 | 빠른 양자화 |
| AWQ | 활성화 인식 | 품질 유지 |
| GGUF/GGML | llama.cpp 포맷 | CPU 추론 |
| bitsandbytes | 8-bit/4-bit 학습 | QLoRA |

### H3. KV Cache 양자화

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| FP8 KV Cache | 메모리 절감 | 긴 컨텍스트 |
| Quantized Attention | 추론 가속 | 처리량 |

---

## 부록 I: 서빙 심화 (Phase 6 연결)

### I1. 추론 최적화

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Prefill vs Decode | 단계별 특성 | 최적화 전략 |
| Speculative Decoding | 추측 생성 | 지연 시간 |
| Continuous Batching | 동적 배치 | 처리량 |

### I2. 메모리 관리

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| PagedAttention | 페이지 단위 KV | vLLM |
| RadixAttention | 접두사 캐싱 | SGLang |
| Prefix Caching | 시스템 프롬프트 재사용 | 효율성 |

### I3. 배포 패턴

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Blue-Green | 무중단 배포 | 안정성 |
| Canary | 점진적 배포 | 리스크 관리 |
| A/B Testing | 모델 비교 | 성능 검증 |

### I4. 모니터링 상세

| 주제 | 내용 | 본문 연결 |
|------|------|----------|
| Prometheus 쿼리 | PromQL 기초 | 메트릭 |
| Grafana 대시보드 | 시각화 설정 | 모니터링 |
| 알림 설정 | Alertmanager | 운영 |

---

## 부록 J: 의료 OCR 특화 (SI 프로젝트 연결)

### J1. 의료 문서 유형별 처리

| 유형 | 특징 | 처리 전략 |
|------|------|----------|
| 진단서 | 양식 + 손글씨 | 영역 분리 |
| 처방전 | 약어, 용량 | 도메인 사전 |
| 영수증 | 테이블 구조 | 구조 추출 |
| 검사결과지 | 참조값 비교 | 수치 해석 |

### J2. 전처리 파이프라인

| 단계 | 내용 | 도구 |
|------|------|------|
| 스캔 보정 | 기울기, 해상도 | OpenCV |
| 노이즈 제거 | 얼룩, 접힘 | 필터링 |
| 이진화 | 배경 분리 | Otsu, Adaptive |
| 영역 검출 | 텍스트/표/이미지 | Layout 분석 |

### J3. 한국어 특화

| 주제 | 내용 | 처리 |
|------|------|------|
| 한글 형태소 | 조사, 어미 | 토큰화 |
| 한자 혼용 | 의학 용어 | 사전 |
| 날짜 형식 | YYYY.MM.DD 등 | 정규식 |
| 금액 표기 | ₩, 원, 콤마 | 파싱 |

### J4. 품질 검증

| 메트릭 | 기준 | 조치 |
|------|------|------|
| CER | < 3% | 재검토 |
| 필수 필드 | 100% | 에러 리포트 |
| 숫자 정확도 | 99.9% | 수동 검증 |

---

## 작업 우선순위

### 1단계: 필수 기초 (Phase 1 지원)
| 순위 | 부록 | 이유 |
|------|------|------|
| 1 | A0_수학_기호_사전 | 모든 챕터 참조 필수 |
| 2 | A2_미적분 (Chain Rule) | Backprop 이해 핵심 |
| 3 | B2_Backpropagation_유도 | 학습 원리 |
| 4 | A1_선형대수 | 행렬 연산 기초 |
| 5 | B1_Attention_수식_유도 | Transformer 핵심 |

### 2단계: Vision & VLM (Phase 2 지원)
| 순위 | 부록 | 이유 |
|------|------|------|
| 6 | E1_CNN_vs_Transformer | 아키텍처 비교 |
| 7 | E3_해상도_처리 | OCR 핵심 |

### 3단계: 실무 (Phase 4-6 지원)
| 순위 | 부록 | 이유 |
|------|------|------|
| 8 | G1_Optimizer_상세 | 학습 최적화 |
| 9 | H2_LLM_양자화 | 추론 효율 |
| 10 | I2_메모리_관리 | 서빙 핵심 |

### 4단계: SI 프로젝트 특화
| 순위 | 부록 | 이유 |
|------|------|------|
| 11 | J1_의료_문서_유형 | 도메인 특화 |
| 12 | J3_한국어_특화 | 로컬라이제이션 |

---

## 본문-부록 연결 방식

본문에서 부록 참조 시:
```markdown
> 💡 상세 유도는 [부록 B2: Backpropagation 유도](../07_부록/B_증명_유도/B2_Backpropagation_유도.md) 참조
```

부록에서 본문 역참조:
```markdown
> 본 내용은 [1.1 수학적 기초](../01_딥러닝_Transformer_기초/01_수학적_기초.md)의 Chain Rule 섹션과 연결됨
```

---

## 예상 분량

| 부록 | 예상 분량 | 포함 내용 |
|------|----------|----------|
| A (수학 기초) | 각 20-30페이지 | 정의, 예제, Python 코드 |
| B (증명 유도) | 각 10-15페이지 | 수식 유도, 그래프 |
| C (구현 상세) | 각 30-50페이지 | 전체 코드, 주석 |
| D (실습 환경) | 각 5-10페이지 | 명령어, 설정 파일 |

**총 예상 분량**: 약 200-300페이지

---

## 다음 단계

1. **폴더 구조 생성**: `A_수학_기초/`, `B_증명_유도/` 등
2. **A2_미적분.md 작성**: Chain Rule 상세 (우선순위 1)
3. **본문 업데이트**: 부록 참조 링크 추가
4. **검증**: 수식 정확성, 코드 실행 가능성

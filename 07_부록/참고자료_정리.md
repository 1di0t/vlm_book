# 부록: 참고자료 종합 정리

## 핵심 논문 목록

### Phase 1: 딥러닝 & Transformer 기초

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **Attention Is All You Need** | Vaswani et al. | 2017 | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) | Transformer 아키텍처 |
| **BERT** | Devlin et al. | 2018 | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) | Bidirectional pretraining |
| **GPT-3** | Brown et al. | 2020 | [arXiv:2005.14165](https://arxiv.org/abs/2005.14165) | Few-shot learning |
| **RoFormer (RoPE)** | Su et al. | 2021 | [arXiv:2104.09864](https://arxiv.org/abs/2104.09864) | Rotary Position Embedding |
| **Layer Normalization** | Ba et al. | 2016 | [arXiv:1607.06450](https://arxiv.org/abs/1607.06450) | Layer Norm |
| **BPE Tokenization** | Sennrich et al. | 2016 | [arXiv:1508.07909](https://arxiv.org/abs/1508.07909) | Subword 토큰화 |
| **SentencePiece** | Kudo & Richardson | 2018 | [arXiv:1808.06226](https://arxiv.org/abs/1808.06226) | 언어 독립 토큰화 |

### Phase 2: Vision & VLM 아키텍처

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **ViT** | Dosovitskiy et al. | 2020 | [arXiv:2010.11929](https://arxiv.org/abs/2010.11929) | Vision Transformer |
| **DeiT** | Touvron et al. | 2021 | [arXiv:2012.12877](https://arxiv.org/abs/2012.12877) | Data-efficient ViT |
| **Swin Transformer** | Liu et al. | 2021 | [arXiv:2103.14030](https://arxiv.org/abs/2103.14030) | Shifted Window |
| **CLIP** | Radford et al. | 2021 | [arXiv:2103.00020](https://arxiv.org/abs/2103.00020) | Contrastive Vision-Language |
| **Flamingo** | Alayrac et al. | 2022 | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) | Cross-attention VLM |
| **LLaVA** | Liu et al. | 2023 | [arXiv:2304.08485](https://arxiv.org/abs/2304.08485) | Visual Instruction Tuning |
| **LLaVA-1.5** | Liu et al. | 2023 | [arXiv:2310.03744](https://arxiv.org/abs/2310.03744) | Improved baselines |
| **Qwen2-VL** | Wang et al. | 2024 | [arXiv:2409.12191](https://arxiv.org/abs/2409.12191) | Dynamic resolution |
| **InternVL** | Chen et al. | 2024 | [arXiv:2312.14238](https://arxiv.org/abs/2312.14238) | Large-scale VLM |

### Phase 3: 문서 이해 & OCR

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **Nougat** | Blecher et al. | 2023 | [arXiv:2308.13418](https://arxiv.org/abs/2308.13418) | 학술 문서 OCR |
| **Pix2Struct** | Lee et al. | 2022 | [arXiv:2210.03347](https://arxiv.org/abs/2210.03347) | Screenshot parsing |
| **DocOwl** | Ye et al. | 2023 | [arXiv:2307.02499](https://arxiv.org/abs/2307.02499) | OCR-free 문서 이해 |

### Phase 4: Fine-tuning & 학습

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **LoRA** | Hu et al. | 2021 | [arXiv:2106.09685](https://arxiv.org/abs/2106.09685) | Low-rank adaptation |
| **QLoRA** | Dettmers et al. | 2023 | [arXiv:2305.14314](https://arxiv.org/abs/2305.14314) | 4-bit fine-tuning |
| **DeepSpeed ZeRO** | Rasley et al. | 2020 | [Microsoft Research](https://www.microsoft.com/en-us/research/publication/zero-memory-optimizations-toward-training-trillion-parameter-models/) | 분산 학습 |
| **FSDP** | PyTorch | 2023 | [arXiv:2304.11277](https://arxiv.org/abs/2304.11277) | Fully sharded training |
| **FlashAttention** | Dao et al. | 2022 | [arXiv:2205.14135](https://arxiv.org/abs/2205.14135) | IO-aware attention |
| **Mixed Precision** | Micikevicius et al. | 2018 | [arXiv:1710.03740](https://arxiv.org/abs/1710.03740) | FP16 학습 |
| **Gradient Checkpointing** | Chen et al. | 2016 | [arXiv:1604.06174](https://arxiv.org/abs/1604.06174) | 메모리 효율 학습 |

### Phase 5: 양자화 & 최적화

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **AWQ** | Lin et al. | 2023 | [arXiv:2306.00978](https://arxiv.org/abs/2306.00978) | Activation-aware quant |
| **GPTQ** | Frantar et al. | 2022 | [arXiv:2210.17323](https://arxiv.org/abs/2210.17323) | Post-training quant |
| **KV Cache Survey** | - | 2024 | [arXiv:2412.19442](https://arxiv.org/abs/2412.19442) | KV Cache 최적화 |

### Phase 6: 추론 & 배포

| 제목 | 저자 | 연도 | 링크 | 핵심 기여 |
|------|------|------|------|----------|
| **vLLM (PagedAttention)** | Kwon et al. | 2023 | [arXiv:2309.06180](https://arxiv.org/abs/2309.06180) | 효율적 메모리 관리 |
| **SGLang** | Zheng et al. | 2024 | [arXiv:2312.07104](https://arxiv.org/abs/2312.07104) | RadixAttention |

---

## 강의 및 튜토리얼

### 대학 강의

| 강의명 | 기관 | 링크 | 내용 |
|--------|------|------|------|
| **CS224N** | Stanford | https://web.stanford.edu/class/cs224n/ | NLP with Deep Learning |
| **CS231N** | Stanford | https://cs231n.github.io/ | CNN for Visual Recognition |
| **CS229** | Stanford | https://cs229.stanford.edu/ | Machine Learning |

### 온라인 튜토리얼

| 제목 | 저자 | 링크 | 내용 |
|------|------|------|------|
| **Neural Networks: Zero to Hero** | Andrej Karpathy | https://karpathy.ai/zero-to-hero.html | GPT 구현 |
| **The Illustrated Transformer** | Jay Alammar | https://jalammar.github.io/illustrated-transformer/ | Transformer 시각화 |
| **Aman's AI Journal** | Aman Chadha | https://aman.ai/ | VLM 아키텍처 |

---

## GitHub 저장소

### 모델 구현

| 저장소 | 설명 | 링크 |
|--------|------|------|
| **nanoGPT** | 최소 GPT 구현 | https://github.com/karpathy/nanoGPT |
| **minbpe** | BPE 토크나이저 | https://github.com/karpathy/minbpe |
| **LLaVA** | Visual Instruction Tuning | https://github.com/haotian-liu/LLaVA |
| **Qwen2-VL** | Qwen VLM | https://github.com/QwenLM/Qwen2-VL |
| **InternVL** | InternVL 시리즈 | https://github.com/OpenGVLab/InternVL |

### 학습 도구

| 저장소 | 설명 | 링크 |
|--------|------|------|
| **PEFT** | LoRA/QLoRA 구현 | https://github.com/huggingface/peft |
| **DeepSpeed** | 분산 학습 | https://github.com/microsoft/DeepSpeed |
| **bitsandbytes** | 양자화 | https://github.com/TimDettmers/bitsandbytes |
| **flash-attention** | FlashAttention | https://github.com/Dao-AILab/flash-attention |

### 추론 엔진

| 저장소 | 설명 | 링크 |
|--------|------|------|
| **vLLM** | PagedAttention 서빙 | https://github.com/vllm-project/vllm |
| **SGLang** | RadixAttention 서빙 | https://github.com/sgl-project/sglang |
| **TensorRT-LLM** | NVIDIA 최적화 | https://github.com/NVIDIA/TensorRT-LLM |
| **llm-awq** | AWQ 양자화 | https://github.com/mit-han-lab/llm-awq |

---

## 벤치마크 & 데이터셋

### VLM 벤치마크

| 벤치마크 | 설명 | 링크 |
|---------|------|------|
| **MMMU** | 멀티모달 이해 | https://mmmu-benchmark.github.io/ |
| **VQAv2** | Visual QA | https://visualqa.org/ |
| **TextVQA** | 텍스트 포함 이미지 QA | https://textvqa.org/ |
| **DocVQA** | 문서 QA | https://www.docvqa.org/ |
| **ChartQA** | 차트 QA | https://github.com/vis-nlp/ChartQA |

### OCR 데이터셋

| 데이터셋 | 설명 | 용도 |
|---------|------|------|
| **FUNSD** | Form Understanding | 양식 이해 |
| **SROIE** | 영수증 | 정보 추출 |
| **RVL-CDIP** | 문서 분류 | 16 클래스 |
| **CORD** | 영수증 OCR | 한국어 포함 |

---

## 도구 & 라이브러리

### Python 라이브러리

| 라이브러리 | 용도 | 설치 |
|-----------|------|------|
| PyTorch | 딥러닝 프레임워크 | `pip install torch` |
| Transformers | 모델 허브 | `pip install transformers` |
| PEFT | 효율적 파인튜닝 | `pip install peft` |
| vLLM | 추론 서빙 | `pip install vllm` |
| OpenCV | 이미지 처리 | `pip install opencv-python` |
| rapidfuzz | 퍼지 매칭 | `pip install rapidfuzz` |

### 모니터링 도구

| 도구 | 용도 | 링크 |
|------|------|------|
| Weights & Biases | 실험 추적 | https://wandb.ai/ |
| MLflow | ML 라이프사이클 | https://mlflow.org/ |
| Prometheus | 메트릭 수집 | https://prometheus.io/ |
| Grafana | 대시보드 | https://grafana.com/ |

---

## 권장 학습 순서

### 초급 (1-2개월)

1. **딥러닝 기초**
   - CS231N 강의 1-5
   - PyTorch 튜토리얼

2. **Transformer 이해**
   - The Illustrated Transformer
   - Attention Is All You Need 논문

3. **실습**
   - nanoGPT 따라하기
   - HuggingFace 튜토리얼

### 중급 (2-3개월)

1. **Vision Transformer**
   - ViT, Swin Transformer 논문
   - timm 라이브러리 실습

2. **VLM 아키텍처**
   - CLIP, LLaVA 논문
   - LLaVA 코드 분석

3. **Fine-tuning**
   - LoRA 논문 및 PEFT 실습
   - QLoRA 실습

### 고급 (3-4개월)

1. **최적화**
   - FlashAttention, PagedAttention
   - 양자화 (AWQ, GPTQ)

2. **배포**
   - vLLM, SGLang 실습
   - Docker, Kubernetes

3. **프로젝트**
   - 도메인 특화 VLM 파인튜닝
   - 프로덕션 배포

---

## 추가 리소스

### 블로그 & 뉴스레터

- **Hugging Face Blog**: https://huggingface.co/blog
- **LMSYS Org**: https://lmsys.org/blog/
- **The Gradient**: https://thegradient.pub/

### 커뮤니티

- **r/MachineLearning**: https://reddit.com/r/MachineLearning
- **Hugging Face Discord**: https://huggingface.co/join/discord
- **EleutherAI Discord**: https://discord.gg/eleutherai

### 논문 탐색

- **arXiv**: https://arxiv.org/
- **Papers with Code**: https://paperswithcode.com/
- **Semantic Scholar**: https://www.semanticscholar.org/

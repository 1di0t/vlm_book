{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy로 신경망 구현하기\n",
    "\n",
    "이 노트북에서는 NumPy만으로 간단한 신경망을 구현한다.\n",
    "Forward pass, Backward pass (backpropagation), Gradient Descent를 직접 구현해본다.\n",
    "\n",
    "## 학습 목표\n",
    "- 행렬 연산으로 신경망 forward pass 구현\n",
    "- Chain rule을 적용한 backward pass 구현\n",
    "- Gradient descent로 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 활성화 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid 활성화 함수\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid의 도함수\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU 활성화 함수\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLU의 도함수\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax 함수 (수치 안정성 포함)\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error Loss\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    \"\"\"MSE의 도함수\"\"\"\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"Cross-Entropy Loss (softmax 출력 가정)\"\"\"\n",
    "    eps = 1e-10\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=-1))\n",
    "\n",
    "def softmax_cross_entropy_derivative(y_pred, y_true):\n",
    "    \"\"\"Softmax + Cross-Entropy의 combined derivative\"\"\"\n",
    "    return (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망 레이어 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"선형 변환 레이어: y = Wx + b\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Xavier initialization\n",
    "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
    "        self.b = np.zeros(out_features)\n",
    "        \n",
    "        # Gradient 저장용\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Forward pass 중간값 저장 (backward에서 사용)\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: y = Wx + b\"\"\"\n",
    "        self.x = x  # 저장 (backward에서 필요)\n",
    "        return x @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass\n",
    "        \n",
    "        Args:\n",
    "            dout: 출력에 대한 gradient (dL/dy)\n",
    "        \n",
    "        Returns:\n",
    "            dx: 입력에 대한 gradient (dL/dx)\n",
    "        \"\"\"\n",
    "        # dL/dW = x^T @ dL/dy\n",
    "        self.dW = self.x.T @ dout\n",
    "        \n",
    "        # dL/db = sum(dL/dy, axis=0)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        # dL/dx = dL/dy @ W^T\n",
    "        dx = dout @ self.W.T\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"파라미터 업데이트\"\"\"\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU 활성화 레이어\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return relu(x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * relu_derivative(self.x)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass  # 학습할 파라미터 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid 활성화 레이어\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = sigmoid(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # sigmoid의 미분: sigmoid(x) * (1 - sigmoid(x))\n",
    "        return dout * self.out * (1 - self.out)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 신경망 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"다층 퍼셉트론 (MLP)\"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers: 레이어 객체들의 리스트\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass (역순으로)\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"모든 레이어 파라미터 업데이트\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "    \n",
    "    def train_step(self, x, y, lr=0.01):\n",
    "        \"\"\"한 번의 학습 스텝\"\"\"\n",
    "        # Forward\n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = mse_loss(y_pred, y)\n",
    "        \n",
    "        # Backward\n",
    "        dout = mse_loss_derivative(y_pred, y)\n",
    "        self.backward(dout)\n",
    "        \n",
    "        # Update\n",
    "        self.update(lr)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실습: XOR 문제 풀기\n",
    "\n",
    "XOR은 선형으로 분리 불가능한 문제다. 신경망이 필요한 대표적 예시."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "print(\"XOR 데이터:\")\n",
    "for i in range(4):\n",
    "    print(f\"  {X[i]} -> {y[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 생성: 2 -> 8 -> 1\n",
    "model = NeuralNetwork([\n",
    "    Linear(2, 8),    # 입력 2 -> 은닉 8\n",
    "    ReLU(),          # 활성화\n",
    "    Linear(8, 1),    # 은닉 8 -> 출력 1\n",
    "    Sigmoid()        # 0-1 범위로\n",
    "])\n",
    "\n",
    "# 학습\n",
    "losses = []\n",
    "lr = 0.5\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = model.train_step(X, y, lr=lr)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 시각화\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('XOR 학습 곡선')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과\n",
    "print(\"학습 후 예측:\")\n",
    "predictions = model.forward(X)\n",
    "for i in range(4):\n",
    "    pred = predictions[i][0]\n",
    "    print(f\"  {X[i]} -> {pred:.4f} (정답: {y[i][0]}, 반올림: {round(pred)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 실습: MNIST 손글씨 분류 (간단 버전)\n",
    "\n",
    "실제 이미지 분류 문제에 도전해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 MNIST 대체 데이터 생성\n",
    "# (실제로는 sklearn이나 tensorflow에서 로드)\n",
    "\n",
    "def generate_simple_digit_data(n_samples=1000):\n",
    "    \"\"\"간단한 숫자 분류 데이터 생성 (0 vs 1)\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        label = np.random.randint(0, 2)\n",
    "        \n",
    "        # 간단한 패턴: 0은 원형, 1은 세로선\n",
    "        if label == 0:\n",
    "            # 원형 패턴 (대각선 활성화)\n",
    "            pattern = np.array([\n",
    "                0.1, 0.8, 0.8, 0.1,\n",
    "                0.8, 0.1, 0.1, 0.8,\n",
    "                0.8, 0.1, 0.1, 0.8,\n",
    "                0.1, 0.8, 0.8, 0.1\n",
    "            ])\n",
    "        else:\n",
    "            # 세로선 패턴 (중앙 활성화)\n",
    "            pattern = np.array([\n",
    "                0.1, 0.8, 0.8, 0.1,\n",
    "                0.1, 0.8, 0.8, 0.1,\n",
    "                0.1, 0.8, 0.8, 0.1,\n",
    "                0.1, 0.8, 0.8, 0.1\n",
    "            ])\n",
    "        \n",
    "        # 노이즈 추가\n",
    "        noise = np.random.randn(16) * 0.1\n",
    "        X.append(np.clip(pattern + noise, 0, 1))\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터 생성\n",
    "X_train, y_train = generate_simple_digit_data(800)\n",
    "X_test, y_test = generate_simple_digit_data(200)\n",
    "\n",
    "# One-hot encoding\n",
    "def to_onehot(y, num_classes=2):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_oh = to_onehot(y_train)\n",
    "y_test_oh = to_onehot(y_test)\n",
    "\n",
    "print(f\"학습 데이터: {X_train.shape}, {y_train_oh.shape}\")\n",
    "print(f\"테스트 데이터: {X_test.shape}, {y_test_oh.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"Softmax + Cross-Entropy 결합 레이어\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "    \n",
    "    def forward(self, x, y_true):\n",
    "        self.y_pred = softmax(x)\n",
    "        self.y_true = y_true\n",
    "        return cross_entropy_loss(self.y_pred, y_true)\n",
    "    \n",
    "    def backward(self):\n",
    "        return softmax_cross_entropy_derivative(self.y_pred, self.y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"분류를 위한 신경망\"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return softmax(logits)\n",
    "    \n",
    "    def train_step(self, x, y, lr=0.01):\n",
    "        # Forward\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn.forward(logits, y)\n",
    "        \n",
    "        # Backward\n",
    "        dout = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # Update\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        return np.mean(preds == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기 생성\n",
    "classifier = Classifier([\n",
    "    Linear(16, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 2)  # 2개 클래스\n",
    "])\n",
    "\n",
    "# 학습\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Mini-batch 학습\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = X_train[batch_idx]\n",
    "        y_batch = y_train_oh[batch_idx]\n",
    "        \n",
    "        loss = classifier.train_step(X_batch, y_batch, lr=lr)\n",
    "        epoch_loss += loss\n",
    "        n_batches += 1\n",
    "    \n",
    "    epoch_loss /= n_batches\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    train_acc = classifier.accuracy(X_train, y_train_oh)\n",
    "    test_acc = classifier.accuracy(X_test, y_test_oh)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {epoch_loss:.4f} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curve')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient 검증 (Gradient Checking)\n",
    "\n",
    "구현한 backward pass가 올바른지 수치 미분으로 검증한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(model, x, y, param, h=1e-5):\n",
    "    \"\"\"수치 미분으로 gradient 계산\"\"\"\n",
    "    grad = np.zeros_like(param)\n",
    "    \n",
    "    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        original = param[idx]\n",
    "        \n",
    "        # f(x + h)\n",
    "        param[idx] = original + h\n",
    "        loss_plus = mse_loss(model.forward(x), y)\n",
    "        \n",
    "        # f(x - h)\n",
    "        param[idx] = original - h\n",
    "        loss_minus = mse_loss(model.forward(x), y)\n",
    "        \n",
    "        # 원래 값 복원\n",
    "        param[idx] = original\n",
    "        \n",
    "        grad[idx] = (loss_plus - loss_minus) / (2 * h)\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient 검증\n",
    "print(\"Gradient Checking...\")\n",
    "\n",
    "# 작은 네트워크로 테스트\n",
    "test_model = NeuralNetwork([\n",
    "    Linear(2, 3),\n",
    "    Sigmoid(),\n",
    "    Linear(3, 1),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "# 테스트 데이터\n",
    "test_x = np.array([[0.5, 0.3]])\n",
    "test_y = np.array([[0.8]])\n",
    "\n",
    "# Forward & Backward\n",
    "pred = test_model.forward(test_x)\n",
    "dout = mse_loss_derivative(pred, test_y)\n",
    "test_model.backward(dout)\n",
    "\n",
    "# 첫 번째 Linear 레이어의 gradient 비교\n",
    "layer = test_model.layers[0]\n",
    "analytical_grad = layer.dW\n",
    "numerical_grad = numerical_gradient(test_model, test_x, test_y, layer.W)\n",
    "\n",
    "# 상대 오차\n",
    "diff = np.abs(analytical_grad - numerical_grad)\n",
    "rel_error = diff / (np.abs(analytical_grad) + np.abs(numerical_grad) + 1e-8)\n",
    "\n",
    "print(f\"\\nAnalytical gradient:\\n{analytical_grad}\")\n",
    "print(f\"\\nNumerical gradient:\\n{numerical_grad}\")\n",
    "print(f\"\\nMax relative error: {np.max(rel_error):.2e}\")\n",
    "\n",
    "if np.max(rel_error) < 1e-5:\n",
    "    print(\"\\n✓ Gradient check PASSED!\")\n",
    "else:\n",
    "    print(\"\\n✗ Gradient check FAILED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 정리\n",
    "\n",
    "이 노트북에서 배운 내용:\n",
    "\n",
    "1. **Forward Pass**: 입력 → 출력 계산 (행렬 곱 + 활성화)\n",
    "2. **Backward Pass**: Chain Rule로 gradient 전파\n",
    "3. **Gradient Descent**: gradient 반대 방향으로 파라미터 업데이트\n",
    "4. **Gradient Checking**: 수치 미분으로 구현 검증\n",
    "\n",
    "### 핵심 수식\n",
    "\n",
    "- Forward: $y = \\sigma(Wx + b)$\n",
    "- Backward: $\\frac{\\partial L}{\\partial W} = x^T \\frac{\\partial L}{\\partial y} \\cdot \\sigma'$\n",
    "- Update: $W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

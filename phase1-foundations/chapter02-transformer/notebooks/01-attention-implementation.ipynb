{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention 구현\n",
    "\n",
    "PyTorch로 Scaled Dot-Product Attention과 Multi-Head Attention을 직접 구현한다.\n",
    "\n",
    "## 학습 목표\n",
    "- Attention 수식을 코드로 변환\n",
    "- Multi-Head의 병렬 처리 이해\n",
    "- Attention weights 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, ..., seq_len, d_k)\n",
    "        K: Key tensor (batch, ..., seq_len, d_k)\n",
    "        V: Value tensor (batch, ..., seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch, ..., seq_len, d_v)\n",
    "        attention_weights: (batch, ..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # Step 1: Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (optional)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: @ V\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (첫 번째 배치):\")\n",
    "print(attn_weights[0])\n",
    "print(f\"\\n각 행의 합: {attn_weights[0].sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Weights 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens=None, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Attention weights 히트맵 시각화\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if tokens is None:\n",
    "        tokens = [f\"pos_{i}\" for i in range(attention_weights.shape[0])]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        attention_weights.detach().numpy(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        square=True\n",
    "    )\n",
    "    plt.xlabel('Key')\n",
    "    plt.ylabel('Query')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 시각화\n",
    "tokens = [\"I\", \"love\", \"deep\", \"learning\"]\n",
    "visualize_attention(attn_weights[0], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal Mask (Decoder용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Lower triangular mask for causal (autoregressive) attention\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Causal mask 시각화\n",
    "causal_mask = create_causal_mask(4)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "\n",
    "# Causal attention\n",
    "output_causal, attn_weights_causal = scaled_dot_product_attention(Q, K, V, causal_mask)\n",
    "\n",
    "visualize_attention(attn_weights_causal[0], tokens, \"Causal Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention 구현\n",
    "    \n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n",
    "    where head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, -1, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1. Linear projection\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # 2. Split into heads\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. Scaled dot-product attention\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dims\n",
    "        \n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Combine heads\n",
    "        attn_output = self.combine_heads(attn_output, batch_size)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention 테스트\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 4\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 입력 생성\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Self-Attention (Q=K=V=x)\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Head의 Attention 시각화\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "tokens = [\"I\", \"love\", \"deep\", \"learning\"]\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx // 4, head_idx % 4]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        attn_weights[0, head_idx].detach().numpy(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        annot=True,\n",
    "        fmt='.2f'\n",
    "    )\n",
    "    ax.set_title(f'Head {head_idx}')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Weights', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 파라미터 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"모델의 학습 가능한 파라미터 수 계산\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Multi-Head Attention 파라미터\n",
    "mha_params = count_parameters(mha)\n",
    "print(f\"Multi-Head Attention 파라미터 수: {mha_params:,}\")\n",
    "print(f\"\\n이론적 계산:\")\n",
    "print(f\"  W_q: {d_model} x {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_k: {d_model} x {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_v: {d_model} x {d_model} = {d_model * d_model}\")\n",
    "print(f\"  W_o: {d_model} x {d_model} = {d_model * d_model}\")\n",
    "print(f\"  Biases: 4 x {d_model} = {4 * d_model}\")\n",
    "print(f\"  Total: {4 * d_model * d_model + 4 * d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch 내장 구현과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 내장 MultiheadAttention\n",
    "pytorch_mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "# 테스트\n",
    "with torch.no_grad():\n",
    "    pytorch_output, pytorch_weights = pytorch_mha(x, x, x)\n",
    "\n",
    "print(f\"PyTorch MHA output shape: {pytorch_output.shape}\")\n",
    "print(f\"PyTorch MHA weights shape: {pytorch_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaled Dot-Product의 스케일링 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링의 중요성 시각화\n",
    "d_k_values = [8, 64, 512]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for idx, d_k in enumerate(d_k_values):\n",
    "    Q = torch.randn(1, 4, d_k)\n",
    "    K = torch.randn(1, 4, d_k)\n",
    "    \n",
    "    # 스케일링 없이\n",
    "    scores_no_scale = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    attn_no_scale = F.softmax(scores_no_scale, dim=-1)\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    scores_scaled = scores_no_scale / math.sqrt(d_k)\n",
    "    attn_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "    \n",
    "    # 스케일링 없음\n",
    "    axes[0, idx].hist(attn_no_scale[0].flatten().numpy(), bins=20)\n",
    "    axes[0, idx].set_title(f'd_k={d_k}, No Scaling')\n",
    "    axes[0, idx].set_xlabel('Attention Weight')\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    axes[1, idx].hist(attn_scaled[0].flatten().numpy(), bins=20)\n",
    "    axes[1, idx].set_title(f'd_k={d_k}, With Scaling')\n",
    "    axes[1, idx].set_xlabel('Attention Weight')\n",
    "\n",
    "plt.suptitle('Effect of Scaling on Attention Distribution', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"스케일링 없이 d_k가 크면 softmax가 극단적으로 뾰족해짐 (대부분 0 또는 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 정리\n",
    "\n",
    "### 핵심 구현 포인트\n",
    "\n",
    "1. **Scaled Dot-Product Attention**\n",
    "   - `scores = Q @ K^T / sqrt(d_k)`\n",
    "   - `attention = softmax(scores)`\n",
    "   - `output = attention @ V`\n",
    "\n",
    "2. **Multi-Head Attention**\n",
    "   - Linear projection: `Q = xW_q`, `K = xW_k`, `V = xW_v`\n",
    "   - Split into heads: reshape to `(batch, heads, seq_len, d_k)`\n",
    "   - Apply attention per head\n",
    "   - Combine and project: `output = concat(heads)W_o`\n",
    "\n",
    "3. **Masking**\n",
    "   - Causal mask: `torch.tril()` for autoregressive\n",
    "   - Padding mask: `mask == 0` → `-inf`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
---

# 3.1 Tokenization

LLMì€ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ëª»í•œë‹¤. ë¨¼ì € í† í°(ìˆ«ì ID)ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤. ì´ ê³¼ì •ì´ Tokenizationì´ë‹¤.

## 3.1.1 í† í°í™”ì˜ í•„ìš”ì„±

### ì™œ í† í°í™”ê°€ í•„ìš”í•œê°€?

```
í…ìŠ¤íŠ¸: "Hello, world!"
    â†“ Tokenization
í† í° ID: [15496, 11, 995, 0]
    â†“ Embedding
ë²¡í„°: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]
```

ì‹ ê²½ë§ì€ ìˆ«ìë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. í…ìŠ¤íŠ¸ â†’ ìˆ«ì ë³€í™˜ì´ í•„ìˆ˜.

### í† í°í™” ìˆ˜ì¤€

| ìˆ˜ì¤€ | ì˜ˆì‹œ | ì–´íœ˜ í¬ê¸° | ì¥ì  | ë‹¨ì  |
|------|------|----------|------|------|
| Character | H, e, l, l, o | ~100 | OOV ì—†ìŒ | ì‹œí€€ìŠ¤ ê¸¸ì–´ì§ |
| Word | Hello, world | ìˆ˜ë§Œ~ìˆ˜ì‹­ë§Œ | ì˜ë¯¸ ë‹¨ìœ„ | OOV ë¬¸ì œ |
| Subword | Hel, lo, wor, ld | ìˆ˜ë§Œ | ê· í˜• | ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ |

**í˜„ì¬ í‘œì¤€**: Subword í† í°í™” (BPE, WordPiece, SentencePiece)

## 3.1.2 BPE (Byte Pair Encoding)

### ì•Œê³ ë¦¬ì¦˜ ê°œìš”

1. ëª¨ë“  ë‹¨ì–´ë¥¼ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬
2. ê°€ì¥ ë¹ˆë²ˆí•œ ë¬¸ì ìŒì„ ë³‘í•©í•˜ì—¬ ìƒˆ í† í° ìƒì„±
3. ì›í•˜ëŠ” ì–´íœ˜ í¬ê¸°ê¹Œì§€ ë°˜ë³µ

### ì˜ˆì‹œ

```
corpus: "low lower lowest"

Step 0: ì´ˆê¸° ì–´íœ˜ = {l, o, w, e, r, s, t, _}
        í† í°í™”: l o w _, l o w e r _, l o w e s t _

Step 1: ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ = (l, o) â†’ ìƒˆ í† í° 'lo'
        í† í°í™”: lo w _, lo w e r _, lo w e s t _

Step 2: ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ = (lo, w) â†’ ìƒˆ í† í° 'low'
        í† í°í™”: low _, low e r _, low e s t _

Step 3: ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ = (low, _) â†’ ìƒˆ í† í° 'low_'
        ...
```

### êµ¬í˜„

```python
from collections import defaultdict
import re

def get_stats(vocab):
    """ì–´íœ˜ì—ì„œ ì¸ì ‘ í† í° ìŒì˜ ë¹ˆë„ ê³„ì‚°"""
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

def merge_vocab(pair, vocab):
    """ê°€ì¥ ë¹ˆë²ˆí•œ ìŒì„ ë³‘í•©"""
    new_vocab = {}
    bigram = re.escape(' '.join(pair))
    pattern = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in vocab:
        new_word = pattern.sub(''.join(pair), word)
        new_vocab[new_word] = vocab[word]
    return new_vocab

def train_bpe(corpus, num_merges):
    """BPE í•™ìŠµ"""
    # ì´ˆê¸° ì–´íœ˜: ë¬¸ì ë‹¨ìœ„
    vocab = defaultdict(int)
    for word in corpus:
        vocab[' '.join(list(word)) + ' </w>'] += 1

    merges = []

    for i in range(num_merges):
        pairs = get_stats(vocab)
        if not pairs:
            break

        best_pair = max(pairs, key=pairs.get)
        vocab = merge_vocab(best_pair, vocab)
        merges.append(best_pair)

        print(f"Merge {i+1}: {best_pair}")

    return vocab, merges

# ì˜ˆì‹œ
corpus = ["low", "lower", "lowest", "newer", "wider"]
vocab, merges = train_bpe(corpus, num_merges=10)
```

### BPE í† í°í™”

```python
def tokenize_bpe(text, merges):
    """í•™ìŠµëœ BPEë¡œ í† í°í™”"""
    tokens = list(text) + ['</w>']

    for pair in merges:
        i = 0
        while i < len(tokens) - 1:
            if tokens[i] == pair[0] and tokens[i+1] == pair[1]:
                tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]
            else:
                i += 1

    return tokens

# ì˜ˆì‹œ
text = "lowest"
tokens = tokenize_bpe(text, merges)
print(f"'{text}' â†’ {tokens}")
```

## 3.1.3 WordPiece (BERT)

### BPEì™€ì˜ ì°¨ì´

- BPE: ê°€ì¥ ë¹ˆë²ˆí•œ ìŒ ì„ íƒ
- WordPiece: ìš°ë„(likelihood)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ìŒ ì„ íƒ

$$
\text{score}(x, y) = \frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}
$$

### í† í°í™” íŠ¹ì§•

- ë‹¨ì–´ ì‹œì‘ì´ ì•„ë‹Œ ì„œë¸Œì›Œë“œëŠ” `##` ì ‘ë‘ì‚¬
- ì˜ˆ: "unbelievable" â†’ ["un", "##believ", "##able"]

```python
# HuggingFace tokenizers ì‚¬ìš©
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer.tokenize("unbelievable")
print(tokens)  # ['un', '##bel', '##ie', '##va', '##ble']
```

## 3.1.4 SentencePiece

### íŠ¹ì§•

- ì–¸ì–´ ë…ë¦½ì  (ì „ì²˜ë¦¬ ì—†ì´ raw text ì²˜ë¦¬)
- Unigram LM ë˜ëŠ” BPE ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
- ê³µë°±ë„ íŠ¹ìˆ˜ ë¬¸ìë¡œ ì²˜ë¦¬ (â–)

### ì‚¬ìš© ì˜ˆì‹œ

```python
import sentencepiece as spm

# í•™ìŠµ
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='my_tokenizer',
    vocab_size=32000,
    model_type='bpe'  # ë˜ëŠ” 'unigram'
)

# ë¡œë“œ ë° ì‚¬ìš©
sp = spm.SentencePieceProcessor()
sp.load('my_tokenizer.model')

text = "Hello, world!"
tokens = sp.encode_as_pieces(text)
print(tokens)  # ['â–Hello', ',', 'â–world', '!']

ids = sp.encode_as_ids(text)
print(ids)  # [1234, 5, 678, 9]
```

## 3.1.5 í˜„ëŒ€ LLMì˜ í† í¬ë‚˜ì´ì €

### GPT ê³„ì—´ (tiktoken)

```python
import tiktoken

# GPT-4 í† í¬ë‚˜ì´ì €
enc = tiktoken.encoding_for_model("gpt-4")

text = "Hello, world!"
tokens = enc.encode(text)
print(f"í† í° ID: {tokens}")
print(f"í† í° ìˆ˜: {len(tokens)}")

# ë””ì½”ë“œ
decoded = enc.decode(tokens)
print(f"ë³µì›: {decoded}")
```

### LLaMA / Qwen (HuggingFace)

```python
from transformers import AutoTokenizer

# LLaMA í† í¬ë‚˜ì´ì €
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

text = "Hello, world!"
encoded = tokenizer(text, return_tensors="pt")
print(f"input_ids: {encoded['input_ids']}")
print(f"attention_mask: {encoded['attention_mask']}")

# ë””ì½”ë“œ
decoded = tokenizer.decode(encoded['input_ids'][0])
print(f"ë³µì›: {decoded}")
```

## 3.1.6 ì–´íœ˜ í¬ê¸° (Vocab Size)

### íŠ¸ë ˆì´ë“œì˜¤í”„

| ì–´íœ˜ í¬ê¸° | ì‹œí€€ìŠ¤ ê¸¸ì´ | ì„ë² ë”© íŒŒë¼ë¯¸í„° | í¬ê·€ í† í° |
|----------|------------|----------------|----------|
| ì‘ìŒ (8K) | ê¸¸ì–´ì§ | ì ìŒ | ì˜ ë¶„í•´ë¨ |
| í¼ (128K) | ì§§ì•„ì§ | ë§ìŒ | OOV ê°€ëŠ¥ |

### ëª¨ë¸ë³„ ì–´íœ˜ í¬ê¸°

| ëª¨ë¸ | ì–´íœ˜ í¬ê¸° |
|------|----------|
| GPT-2 | 50,257 |
| BERT | 30,522 |
| LLaMA | 32,000 |
| LLaMA 2 | 32,000 |
| Qwen | 151,936 |
| GPT-4 | ~100,000 |

### í•œêµ­ì–´ ê³ ë ¤ì‚¬í•­

```python
# ì˜ì–´ vs í•œêµ­ì–´ í† í° íš¨ìœ¨
text_en = "Hello, how are you?"
text_ko = "ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"

# ì˜ì–´ ìµœì í™” í† í¬ë‚˜ì´ì €
tokens_en = tokenizer.encode(text_en)
tokens_ko = tokenizer.encode(text_ko)

print(f"ì˜ì–´ í† í° ìˆ˜: {len(tokens_en)}")  # ~5
print(f"í•œêµ­ì–´ í† í° ìˆ˜: {len(tokens_ko)}")  # ~15-20 (ë¹„íš¨ìœ¨)
```

ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸(Qwen, Gemma)ì€ í•œêµ­ì–´ í† í°ë„ íš¨ìœ¨ì .

## 3.1.7 íŠ¹ìˆ˜ í† í°

### ì¼ë°˜ì ì¸ íŠ¹ìˆ˜ í† í°

| í† í° | ìš©ë„ |
|------|------|
| `<bos>` / `<s>` | ì‹œí€€ìŠ¤ ì‹œì‘ |
| `<eos>` / `</s>` | ì‹œí€€ìŠ¤ ë |
| `<pad>` | íŒ¨ë”© |
| `<unk>` | ë¯¸ë“±ë¡ í† í° |
| `<mask>` | ë§ˆìŠ¤í‚¹ (BERT) |

### ì±— ëª¨ë¸ íŠ¹ìˆ˜ í† í°

```python
# LLaMA 2 Chat í˜•ì‹
prompt = """<s>[INST] <<SYS>>
You are a helpful assistant.
<</SYS>>

What is the capital of France? [/INST]"""

# Qwen Chat í˜•ì‹
prompt = """<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is the capital of France?<|im_end|>
<|im_start|>assistant
"""
```

## 3.1.8 í† í°í™” ì£¼ì˜ì‚¬í•­

### 1. ê³µë°± ì²˜ë¦¬

```python
text1 = "hello"
text2 = " hello"  # ì•ì— ê³µë°±

tokens1 = tokenizer.encode(text1)
tokens2 = tokenizer.encode(text2)

# ë‹¤ë¥¸ í† í°ì´ ë  ìˆ˜ ìˆìŒ!
print(f"'hello' â†’ {tokens1}")
print(f"' hello' â†’ {tokens2}")
```

### 2. íŠ¹ìˆ˜ ë¬¸ì

```python
# ì´ëª¨ì§€, íŠ¹ìˆ˜ ê¸°í˜¸ ì²˜ë¦¬
text = "Hello ğŸ‘‹ World! ğŸŒ"
tokens = tokenizer.encode(text)
print(f"ì´ëª¨ì§€ í¬í•¨: {len(tokens)} í† í°")
```

### 3. ìˆ«ì

```python
# ìˆ«ìëŠ” ìë¦¿ìˆ˜ë³„ë¡œ í† í°í™”ë  ìˆ˜ ìˆìŒ
numbers = ["123", "1234567890", "3.14159"]
for num in numbers:
    tokens = tokenizer.encode(num)
    print(f"{num} â†’ {len(tokens)} í† í°")
```

## 3.1.9 ì‹¤ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] BPE ì•Œê³ ë¦¬ì¦˜ ì§ì ‘ êµ¬í˜„
- [ ] HuggingFace í† í¬ë‚˜ì´ì € ì‚¬ìš©ë²• ìµíˆê¸°
- [ ] ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë¹„êµ
- [ ] íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ ì´í•´
- [ ] í•œêµ­ì–´/ì˜ì–´ í† í° íš¨ìœ¨ ë¹„êµ

## 3.1.10 í•µì‹¬ ìš”ì•½

| ê°œë… | ì„¤ëª… |
|------|------|
| BPE | ë¹ˆë²ˆí•œ ìŒ ë³‘í•© (GPT ê³„ì—´) |
| WordPiece | ìš°ë„ ê¸°ë°˜ ë³‘í•© (BERT) |
| SentencePiece | ì–¸ì–´ ë…ë¦½ì , raw text |
| Vocab Size | í† í° ìˆ˜ vs íŒŒë¼ë¯¸í„° íŠ¸ë ˆì´ë“œì˜¤í”„ |
| íŠ¹ìˆ˜ í† í° | BOS, EOS, PAD, UNK |

## ë‹¤ìŒ ë‹¨ê³„

[3.2 Autoregressive Generation](02-autoregressive.md)ì—ì„œ í† í° ìƒì„± ê³¼ì •ì„ ë‹¤ë£¬ë‹¤.
